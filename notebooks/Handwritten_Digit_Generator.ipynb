{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digit Generator Web App in Google Colab\n",
        "\n",
        "**This script will:**\n",
        "- 1. Install necessary libraries (PyTorch, Streamlit, pyngrok).\n",
        "- 2. Define and train a Conditional DCGAN on the MNIST dataset.\n",
        "- 3. Save the trained Generator model.\n",
        "- 4. Create a Streamlit web application script."
      ],
      "metadata": {
        "id": "gfFW7g1LR7Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install necessary libraries ---\n",
        "# This part needs to be run only once.\n",
        "!pip install torch torchvision streamlit pillow --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "77L_ifReFkX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Device configuration and Hyperparameters ---\n",
        "# This line will automatically detect and use a CUDA-enabled GPU (like T4) if available.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters for GAN training\n",
        "latent_dim = 100\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "num_epochs = 200\n",
        "batch_size = 256\n",
        "lr = 0.0001\n",
        "beta1 = 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHCVGxj9F4X5",
        "outputId": "04f6bab8-ba55-4569-ccfd-1d245cb93462"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Data Transformation and Loading ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# MNIST Dataset\n",
        "print(\"Downloading MNIST dataset...\")\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "dataloader = torch.utils.data.DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
        "print(\"MNIST dataset loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HtCOKFeF5IX",
        "outputId": "6f14ac3d-9fdf-4d1e-a830-b852e0459f52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MNIST dataset...\n",
            "MNIST dataset loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  --- 4. Generator and Discriminator Architectures ---\n",
        "# Generator model for generating images from latent vectors and digit labels\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Embedding for conditional input (digit label)\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "\n",
        "        # Main sequential block of the generator\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: (latent_dim + num_classes) x 1 x 1 (concatenated noise and label)\n",
        "            # From 1x1 to 7x7\n",
        "            nn.ConvTranspose2d(latent_dim + num_classes, 256, 7, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # State size: 256 x 7 x 7\n",
        "            # From 7x7 to 14x14\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # State size: 128 x 14 x 14\n",
        "            # From 14x14 to 28x28\n",
        "            nn.ConvTranspose2d(128, 1, 4, 2, 1, bias=False), # Output 1 channel for grayscale\n",
        "            nn.Tanh() # Output pixel values in [-1, 1]\n",
        "            # Output size: 1 x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        # Combine noise and label embedding\n",
        "        # Label embedding is converted to a vector and concatenated with noise\n",
        "        gen_input = torch.cat((self.label_emb(labels), noise.view(noise.size(0), -1)), -1)\n",
        "        # Reshape for ConvTranspose2d (batch_size, channels, 1, 1)\n",
        "        gen_input = gen_input.view(gen_input.size(0), gen_input.size(1), 1, 1)\n",
        "        return self.main(gen_input)\n",
        "\n",
        "# Discriminator model for distinguishing real from fake images\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Embedding for conditional input (digit label)\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "\n",
        "        # Main sequential block of the discriminator\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: (1 + num_classes) x 28 x 28 (after concatenating label)\n",
        "            nn.Conv2d(1 + num_classes, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size: 64 x 14 x 14\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size: 128 x 7 x 7\n",
        "            nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size: 256 x 4 x 4\n",
        "            nn.Conv2d(256, 1, 4, 1, 0, bias=False), # Output 1 channel for binary classification\n",
        "            nn.Sigmoid() # Output probability in [0, 1]\n",
        "            # Output size: 1 x 1 x 1\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        # Expand labels to match image dimensions for concatenation\n",
        "        labels_expanded = self.label_emb(labels).view(labels.size(0), num_classes, 1, 1).repeat(1, 1, image_size, image_size)\n",
        "        # Concatenate image and expanded label\n",
        "        d_in = torch.cat((img, labels_expanded), 1)\n",
        "        return self.main(d_in).view(-1, 1) # Flatten output for BCE loss"
      ],
      "metadata": {
        "id": "wXkdLGRNF615"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Initialize models and weights ---\n",
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Custom weights initialization for DCGAN\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DK9RmFaGA78",
        "outputId": "782eb06e-8a37-4c0f-91bf-467e0549fd0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (label_emb): Embedding(10, 10)\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(11, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (9): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Loss function and Optimizers ---\n",
        "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "SmqN_-doGCuv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Training Loop ---\n",
        "print(\"\\nStarting Training Loop...\")\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, labels) in enumerate(dataloader):\n",
        "        # Move data to the selected device (GPU if available)\n",
        "        real_images = real_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        netD.zero_grad()\n",
        "        # Train with real images\n",
        "        output_real = netD(real_images, labels).view(-1) # Flatten output for criterion\n",
        "        errD_real = criterion(output_real, torch.ones_like(output_real))\n",
        "        errD_real.backward()\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
        "        fake_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n",
        "        fake_images = netG(noise, fake_labels)\n",
        "        # Train with fake images\n",
        "        output_fake = netD(fake_images.detach(), fake_labels).view(-1) # Detach to prevent G from learning\n",
        "        errD_fake = criterion(output_fake, torch.zeros_like(output_fake))\n",
        "        errD_fake.backward()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Train Generator\n",
        "        netG.zero_grad()\n",
        "        output_gen = netD(fake_images, fake_labels).view(-1)\n",
        "        errG = criterion(output_gen, torch.ones_like(output_gen)) # Generator wants D to classify fakes as real\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
        "                  f\"Loss D: {errD.item():.4f} Loss G: {errG.item():.4f}\")\n",
        "\n",
        "    # Optionally save generator model at certain epochs for checkpoints\n",
        "    # if (epoch + 1) % 10 == 0:\n",
        "    #     torch.save(netG.state_dict(), f\"generator_epoch_{epoch+1}.pth\")\n",
        "    #     print(f\"Generator model saved at epoch {epoch+1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St9AdNB-GE0A",
        "outputId": "0a8f6d89-3d83-4fc9-f1f4-44abab3e169c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Training Loop...\n",
            "Epoch [1/200] Batch [0/469] Loss D: 1.5969 Loss G: 0.7121\n",
            "Epoch [1/200] Batch [100/469] Loss D: 0.7971 Loss G: 1.4931\n",
            "Epoch [1/200] Batch [200/469] Loss D: 0.8702 Loss G: 1.4949\n",
            "Epoch [1/200] Batch [300/469] Loss D: 1.3534 Loss G: 0.6256\n",
            "Epoch [1/200] Batch [400/469] Loss D: 0.9888 Loss G: 0.9355\n",
            "Epoch [2/200] Batch [0/469] Loss D: 0.9804 Loss G: 1.2319\n",
            "Epoch [2/200] Batch [100/469] Loss D: 0.8839 Loss G: 1.0952\n",
            "Epoch [2/200] Batch [200/469] Loss D: 0.7474 Loss G: 1.3542\n",
            "Epoch [2/200] Batch [300/469] Loss D: 1.0748 Loss G: 1.0636\n",
            "Epoch [2/200] Batch [400/469] Loss D: 0.9844 Loss G: 1.1816\n",
            "Epoch [3/200] Batch [0/469] Loss D: 2.5184 Loss G: 0.8988\n",
            "Epoch [3/200] Batch [100/469] Loss D: 1.0732 Loss G: 1.0243\n",
            "Epoch [3/200] Batch [200/469] Loss D: 0.9942 Loss G: 1.2419\n",
            "Epoch [3/200] Batch [300/469] Loss D: 0.8452 Loss G: 1.4341\n",
            "Epoch [3/200] Batch [400/469] Loss D: 1.6712 Loss G: 0.8547\n",
            "Epoch [4/200] Batch [0/469] Loss D: 1.2470 Loss G: 1.4087\n",
            "Epoch [4/200] Batch [100/469] Loss D: 1.1422 Loss G: 1.2657\n",
            "Epoch [4/200] Batch [200/469] Loss D: 0.9528 Loss G: 1.3788\n",
            "Epoch [4/200] Batch [300/469] Loss D: 1.2792 Loss G: 0.9526\n",
            "Epoch [4/200] Batch [400/469] Loss D: 1.3554 Loss G: 1.1932\n",
            "Epoch [5/200] Batch [0/469] Loss D: 0.8001 Loss G: 1.6543\n",
            "Epoch [5/200] Batch [100/469] Loss D: 0.8589 Loss G: 1.2678\n",
            "Epoch [5/200] Batch [200/469] Loss D: 1.2138 Loss G: 1.3943\n",
            "Epoch [5/200] Batch [300/469] Loss D: 1.6101 Loss G: 0.9010\n",
            "Epoch [5/200] Batch [400/469] Loss D: 0.8419 Loss G: 0.9396\n",
            "Epoch [6/200] Batch [0/469] Loss D: 1.0331 Loss G: 1.1793\n",
            "Epoch [6/200] Batch [100/469] Loss D: 0.6950 Loss G: 1.5441\n",
            "Epoch [6/200] Batch [200/469] Loss D: 1.1183 Loss G: 1.7645\n",
            "Epoch [6/200] Batch [300/469] Loss D: 0.8403 Loss G: 0.9130\n",
            "Epoch [6/200] Batch [400/469] Loss D: 0.9751 Loss G: 1.0162\n",
            "Epoch [7/200] Batch [0/469] Loss D: 0.7836 Loss G: 1.4349\n",
            "Epoch [7/200] Batch [100/469] Loss D: 1.6045 Loss G: 0.8882\n",
            "Epoch [7/200] Batch [200/469] Loss D: 1.1306 Loss G: 0.9741\n",
            "Epoch [7/200] Batch [300/469] Loss D: 1.3463 Loss G: 0.8631\n",
            "Epoch [7/200] Batch [400/469] Loss D: 1.2040 Loss G: 0.9992\n",
            "Epoch [8/200] Batch [0/469] Loss D: 1.5744 Loss G: 0.7667\n",
            "Epoch [8/200] Batch [100/469] Loss D: 1.9184 Loss G: 0.7147\n",
            "Epoch [8/200] Batch [200/469] Loss D: 1.0808 Loss G: 1.1137\n",
            "Epoch [8/200] Batch [300/469] Loss D: 1.0504 Loss G: 1.1777\n",
            "Epoch [8/200] Batch [400/469] Loss D: 1.6688 Loss G: 0.7328\n",
            "Epoch [9/200] Batch [0/469] Loss D: 1.0214 Loss G: 1.1011\n",
            "Epoch [9/200] Batch [100/469] Loss D: 1.5200 Loss G: 1.0374\n",
            "Epoch [9/200] Batch [200/469] Loss D: 1.0736 Loss G: 1.0209\n",
            "Epoch [9/200] Batch [300/469] Loss D: 1.2768 Loss G: 0.8469\n",
            "Epoch [9/200] Batch [400/469] Loss D: 1.8729 Loss G: 0.4883\n",
            "Epoch [10/200] Batch [0/469] Loss D: 1.4969 Loss G: 0.9175\n",
            "Epoch [10/200] Batch [100/469] Loss D: 1.6305 Loss G: 0.9766\n",
            "Epoch [10/200] Batch [200/469] Loss D: 1.0986 Loss G: 0.9455\n",
            "Epoch [10/200] Batch [300/469] Loss D: 1.4690 Loss G: 0.9119\n",
            "Epoch [10/200] Batch [400/469] Loss D: 1.1654 Loss G: 1.2955\n",
            "Epoch [11/200] Batch [0/469] Loss D: 1.1432 Loss G: 1.1694\n",
            "Epoch [11/200] Batch [100/469] Loss D: 1.1174 Loss G: 1.1688\n",
            "Epoch [11/200] Batch [200/469] Loss D: 1.1331 Loss G: 0.9326\n",
            "Epoch [11/200] Batch [300/469] Loss D: 1.1459 Loss G: 1.0616\n",
            "Epoch [11/200] Batch [400/469] Loss D: 1.1568 Loss G: 0.8190\n",
            "Epoch [12/200] Batch [0/469] Loss D: 1.0577 Loss G: 0.9957\n",
            "Epoch [12/200] Batch [100/469] Loss D: 1.0062 Loss G: 1.2293\n",
            "Epoch [12/200] Batch [200/469] Loss D: 1.2239 Loss G: 0.9121\n",
            "Epoch [12/200] Batch [300/469] Loss D: 1.6451 Loss G: 0.5325\n",
            "Epoch [12/200] Batch [400/469] Loss D: 1.4202 Loss G: 1.0050\n",
            "Epoch [13/200] Batch [0/469] Loss D: 1.6058 Loss G: 0.8684\n",
            "Epoch [13/200] Batch [100/469] Loss D: 1.0220 Loss G: 1.2085\n",
            "Epoch [13/200] Batch [200/469] Loss D: 1.4164 Loss G: 0.9609\n",
            "Epoch [13/200] Batch [300/469] Loss D: 1.2377 Loss G: 0.9214\n",
            "Epoch [13/200] Batch [400/469] Loss D: 1.1999 Loss G: 1.3495\n",
            "Epoch [14/200] Batch [0/469] Loss D: 1.6907 Loss G: 0.9379\n",
            "Epoch [14/200] Batch [100/469] Loss D: 1.2342 Loss G: 1.1131\n",
            "Epoch [14/200] Batch [200/469] Loss D: 1.6057 Loss G: 0.7364\n",
            "Epoch [14/200] Batch [300/469] Loss D: 1.4111 Loss G: 0.8808\n",
            "Epoch [14/200] Batch [400/469] Loss D: 1.3444 Loss G: 0.7808\n",
            "Epoch [15/200] Batch [0/469] Loss D: 1.7886 Loss G: 0.6689\n",
            "Epoch [15/200] Batch [100/469] Loss D: 1.0772 Loss G: 1.2703\n",
            "Epoch [15/200] Batch [200/469] Loss D: 1.0981 Loss G: 1.3001\n",
            "Epoch [15/200] Batch [300/469] Loss D: 1.0348 Loss G: 1.0158\n",
            "Epoch [15/200] Batch [400/469] Loss D: 1.2485 Loss G: 0.9660\n",
            "Epoch [16/200] Batch [0/469] Loss D: 1.2509 Loss G: 1.0944\n",
            "Epoch [16/200] Batch [100/469] Loss D: 1.0714 Loss G: 1.2125\n",
            "Epoch [16/200] Batch [200/469] Loss D: 1.4278 Loss G: 0.9451\n",
            "Epoch [16/200] Batch [300/469] Loss D: 1.3261 Loss G: 0.9680\n",
            "Epoch [16/200] Batch [400/469] Loss D: 1.5142 Loss G: 0.9010\n",
            "Epoch [17/200] Batch [0/469] Loss D: 0.8385 Loss G: 1.3142\n",
            "Epoch [17/200] Batch [100/469] Loss D: 1.3013 Loss G: 0.8980\n",
            "Epoch [17/200] Batch [200/469] Loss D: 1.6487 Loss G: 0.9781\n",
            "Epoch [17/200] Batch [300/469] Loss D: 0.7843 Loss G: 1.1379\n",
            "Epoch [17/200] Batch [400/469] Loss D: 1.2881 Loss G: 1.0386\n",
            "Epoch [18/200] Batch [0/469] Loss D: 1.6408 Loss G: 0.9205\n",
            "Epoch [18/200] Batch [100/469] Loss D: 1.6799 Loss G: 0.9790\n",
            "Epoch [18/200] Batch [200/469] Loss D: 1.0768 Loss G: 1.1630\n",
            "Epoch [18/200] Batch [300/469] Loss D: 1.2846 Loss G: 0.8882\n",
            "Epoch [18/200] Batch [400/469] Loss D: 1.1673 Loss G: 1.0475\n",
            "Epoch [19/200] Batch [0/469] Loss D: 1.6403 Loss G: 0.6180\n",
            "Epoch [19/200] Batch [100/469] Loss D: 0.9787 Loss G: 1.3938\n",
            "Epoch [19/200] Batch [200/469] Loss D: 1.5518 Loss G: 0.8962\n",
            "Epoch [19/200] Batch [300/469] Loss D: 1.1241 Loss G: 0.8179\n",
            "Epoch [19/200] Batch [400/469] Loss D: 1.2168 Loss G: 1.2730\n",
            "Epoch [20/200] Batch [0/469] Loss D: 0.7560 Loss G: 1.3529\n",
            "Epoch [20/200] Batch [100/469] Loss D: 1.5316 Loss G: 0.7622\n",
            "Epoch [20/200] Batch [200/469] Loss D: 1.0262 Loss G: 0.9246\n",
            "Epoch [20/200] Batch [300/469] Loss D: 1.1132 Loss G: 1.0561\n",
            "Epoch [20/200] Batch [400/469] Loss D: 0.9898 Loss G: 1.1270\n",
            "Epoch [21/200] Batch [0/469] Loss D: 1.6541 Loss G: 0.7502\n",
            "Epoch [21/200] Batch [100/469] Loss D: 0.9801 Loss G: 1.1620\n",
            "Epoch [21/200] Batch [200/469] Loss D: 1.1981 Loss G: 0.8771\n",
            "Epoch [21/200] Batch [300/469] Loss D: 0.9060 Loss G: 1.1561\n",
            "Epoch [21/200] Batch [400/469] Loss D: 0.8058 Loss G: 1.7436\n",
            "Epoch [22/200] Batch [0/469] Loss D: 1.5125 Loss G: 0.6836\n",
            "Epoch [22/200] Batch [100/469] Loss D: 1.2634 Loss G: 1.1374\n",
            "Epoch [22/200] Batch [200/469] Loss D: 1.4481 Loss G: 0.8808\n",
            "Epoch [22/200] Batch [300/469] Loss D: 1.1124 Loss G: 1.5475\n",
            "Epoch [22/200] Batch [400/469] Loss D: 1.2436 Loss G: 1.0955\n",
            "Epoch [23/200] Batch [0/469] Loss D: 1.3878 Loss G: 0.9550\n",
            "Epoch [23/200] Batch [100/469] Loss D: 1.2488 Loss G: 0.9225\n",
            "Epoch [23/200] Batch [200/469] Loss D: 1.2058 Loss G: 1.2620\n",
            "Epoch [23/200] Batch [300/469] Loss D: 1.3340 Loss G: 1.1747\n",
            "Epoch [23/200] Batch [400/469] Loss D: 0.8643 Loss G: 1.2031\n",
            "Epoch [24/200] Batch [0/469] Loss D: 1.8832 Loss G: 0.6337\n",
            "Epoch [24/200] Batch [100/469] Loss D: 1.2541 Loss G: 1.0017\n",
            "Epoch [24/200] Batch [200/469] Loss D: 0.9517 Loss G: 0.9824\n",
            "Epoch [24/200] Batch [300/469] Loss D: 0.6973 Loss G: 1.5678\n",
            "Epoch [24/200] Batch [400/469] Loss D: 2.4818 Loss G: 0.4645\n",
            "Epoch [25/200] Batch [0/469] Loss D: 1.1467 Loss G: 1.0893\n",
            "Epoch [25/200] Batch [100/469] Loss D: 0.7478 Loss G: 1.2820\n",
            "Epoch [25/200] Batch [200/469] Loss D: 1.6004 Loss G: 0.7324\n",
            "Epoch [25/200] Batch [300/469] Loss D: 1.4228 Loss G: 0.7596\n",
            "Epoch [25/200] Batch [400/469] Loss D: 0.8243 Loss G: 1.4574\n",
            "Epoch [26/200] Batch [0/469] Loss D: 1.3171 Loss G: 1.0908\n",
            "Epoch [26/200] Batch [100/469] Loss D: 0.8926 Loss G: 1.3599\n",
            "Epoch [26/200] Batch [200/469] Loss D: 1.0047 Loss G: 1.0961\n",
            "Epoch [26/200] Batch [300/469] Loss D: 0.6094 Loss G: 1.7049\n",
            "Epoch [26/200] Batch [400/469] Loss D: 0.6671 Loss G: 1.6110\n",
            "Epoch [27/200] Batch [0/469] Loss D: 1.2783 Loss G: 0.8461\n",
            "Epoch [27/200] Batch [100/469] Loss D: 1.1361 Loss G: 1.1443\n",
            "Epoch [27/200] Batch [200/469] Loss D: 0.9429 Loss G: 1.3891\n",
            "Epoch [27/200] Batch [300/469] Loss D: 0.9603 Loss G: 1.0923\n",
            "Epoch [27/200] Batch [400/469] Loss D: 1.1346 Loss G: 1.2336\n",
            "Epoch [28/200] Batch [0/469] Loss D: 0.8657 Loss G: 1.0323\n",
            "Epoch [28/200] Batch [100/469] Loss D: 0.8799 Loss G: 1.0624\n",
            "Epoch [28/200] Batch [200/469] Loss D: 1.2926 Loss G: 1.3074\n",
            "Epoch [28/200] Batch [300/469] Loss D: 1.1073 Loss G: 0.8555\n",
            "Epoch [28/200] Batch [400/469] Loss D: 1.3218 Loss G: 1.1558\n",
            "Epoch [29/200] Batch [0/469] Loss D: 1.3029 Loss G: 0.8718\n",
            "Epoch [29/200] Batch [100/469] Loss D: 1.3876 Loss G: 1.0039\n",
            "Epoch [29/200] Batch [200/469] Loss D: 0.7125 Loss G: 1.6721\n",
            "Epoch [29/200] Batch [300/469] Loss D: 1.4157 Loss G: 0.9689\n",
            "Epoch [29/200] Batch [400/469] Loss D: 1.4165 Loss G: 0.9661\n",
            "Epoch [30/200] Batch [0/469] Loss D: 1.0987 Loss G: 0.9439\n",
            "Epoch [30/200] Batch [100/469] Loss D: 0.6403 Loss G: 1.6545\n",
            "Epoch [30/200] Batch [200/469] Loss D: 0.9632 Loss G: 1.1695\n",
            "Epoch [30/200] Batch [300/469] Loss D: 1.4719 Loss G: 0.8348\n",
            "Epoch [30/200] Batch [400/469] Loss D: 0.9093 Loss G: 1.3596\n",
            "Epoch [31/200] Batch [0/469] Loss D: 1.4277 Loss G: 0.9075\n",
            "Epoch [31/200] Batch [100/469] Loss D: 1.0793 Loss G: 1.1063\n",
            "Epoch [31/200] Batch [200/469] Loss D: 1.2330 Loss G: 1.1175\n",
            "Epoch [31/200] Batch [300/469] Loss D: 0.9369 Loss G: 1.4758\n",
            "Epoch [31/200] Batch [400/469] Loss D: 1.2688 Loss G: 0.8240\n",
            "Epoch [32/200] Batch [0/469] Loss D: 1.3619 Loss G: 0.9547\n",
            "Epoch [32/200] Batch [100/469] Loss D: 0.7531 Loss G: 1.5531\n",
            "Epoch [32/200] Batch [200/469] Loss D: 1.2340 Loss G: 0.9805\n",
            "Epoch [32/200] Batch [300/469] Loss D: 1.2218 Loss G: 1.0558\n",
            "Epoch [32/200] Batch [400/469] Loss D: 1.1813 Loss G: 1.0411\n",
            "Epoch [33/200] Batch [0/469] Loss D: 0.9464 Loss G: 1.3285\n",
            "Epoch [33/200] Batch [100/469] Loss D: 1.5800 Loss G: 0.9004\n",
            "Epoch [33/200] Batch [200/469] Loss D: 1.5397 Loss G: 1.0117\n",
            "Epoch [33/200] Batch [300/469] Loss D: 1.5802 Loss G: 1.0267\n",
            "Epoch [33/200] Batch [400/469] Loss D: 0.9682 Loss G: 1.6070\n",
            "Epoch [34/200] Batch [0/469] Loss D: 1.0794 Loss G: 1.0643\n",
            "Epoch [34/200] Batch [100/469] Loss D: 0.7727 Loss G: 1.2887\n",
            "Epoch [34/200] Batch [200/469] Loss D: 0.9807 Loss G: 1.0276\n",
            "Epoch [34/200] Batch [300/469] Loss D: 1.1324 Loss G: 1.1266\n",
            "Epoch [34/200] Batch [400/469] Loss D: 0.9332 Loss G: 1.1304\n",
            "Epoch [35/200] Batch [0/469] Loss D: 0.9107 Loss G: 1.1518\n",
            "Epoch [35/200] Batch [100/469] Loss D: 0.5326 Loss G: 1.9778\n",
            "Epoch [35/200] Batch [200/469] Loss D: 0.8657 Loss G: 1.5590\n",
            "Epoch [35/200] Batch [300/469] Loss D: 0.9739 Loss G: 1.3750\n",
            "Epoch [35/200] Batch [400/469] Loss D: 1.0449 Loss G: 1.2809\n",
            "Epoch [36/200] Batch [0/469] Loss D: 0.8314 Loss G: 1.4975\n",
            "Epoch [36/200] Batch [100/469] Loss D: 0.5583 Loss G: 1.6610\n",
            "Epoch [36/200] Batch [200/469] Loss D: 0.9199 Loss G: 1.4308\n",
            "Epoch [36/200] Batch [300/469] Loss D: 0.9430 Loss G: 1.2240\n",
            "Epoch [36/200] Batch [400/469] Loss D: 0.8818 Loss G: 1.1656\n",
            "Epoch [37/200] Batch [0/469] Loss D: 1.3813 Loss G: 0.8341\n",
            "Epoch [37/200] Batch [100/469] Loss D: 1.8925 Loss G: 0.8276\n",
            "Epoch [37/200] Batch [200/469] Loss D: 1.3601 Loss G: 1.1658\n",
            "Epoch [37/200] Batch [300/469] Loss D: 1.0003 Loss G: 1.3711\n",
            "Epoch [37/200] Batch [400/469] Loss D: 1.5459 Loss G: 0.9052\n",
            "Epoch [38/200] Batch [0/469] Loss D: 1.4131 Loss G: 0.9570\n",
            "Epoch [38/200] Batch [100/469] Loss D: 0.9822 Loss G: 1.5250\n",
            "Epoch [38/200] Batch [200/469] Loss D: 1.0062 Loss G: 0.8834\n",
            "Epoch [38/200] Batch [300/469] Loss D: 0.7060 Loss G: 1.9270\n",
            "Epoch [38/200] Batch [400/469] Loss D: 0.7761 Loss G: 1.4776\n",
            "Epoch [39/200] Batch [0/469] Loss D: 0.6700 Loss G: 1.8501\n",
            "Epoch [39/200] Batch [100/469] Loss D: 1.3125 Loss G: 1.1308\n",
            "Epoch [39/200] Batch [200/469] Loss D: 1.0882 Loss G: 1.3262\n",
            "Epoch [39/200] Batch [300/469] Loss D: 1.2072 Loss G: 1.3339\n",
            "Epoch [39/200] Batch [400/469] Loss D: 0.7595 Loss G: 1.5607\n",
            "Epoch [40/200] Batch [0/469] Loss D: 0.7658 Loss G: 1.6237\n",
            "Epoch [40/200] Batch [100/469] Loss D: 0.7323 Loss G: 1.5955\n",
            "Epoch [40/200] Batch [200/469] Loss D: 1.4311 Loss G: 0.8870\n",
            "Epoch [40/200] Batch [300/469] Loss D: 0.9863 Loss G: 1.2107\n",
            "Epoch [40/200] Batch [400/469] Loss D: 1.1181 Loss G: 1.7558\n",
            "Epoch [41/200] Batch [0/469] Loss D: 1.7832 Loss G: 1.0695\n",
            "Epoch [41/200] Batch [100/469] Loss D: 0.9801 Loss G: 1.0300\n",
            "Epoch [41/200] Batch [200/469] Loss D: 0.7561 Loss G: 1.1130\n",
            "Epoch [41/200] Batch [300/469] Loss D: 1.1460 Loss G: 1.0481\n",
            "Epoch [41/200] Batch [400/469] Loss D: 0.9553 Loss G: 2.0634\n",
            "Epoch [42/200] Batch [0/469] Loss D: 1.1053 Loss G: 1.4639\n",
            "Epoch [42/200] Batch [100/469] Loss D: 1.4180 Loss G: 1.2606\n",
            "Epoch [42/200] Batch [200/469] Loss D: 1.2854 Loss G: 1.0104\n",
            "Epoch [42/200] Batch [300/469] Loss D: 1.4042 Loss G: 0.9950\n",
            "Epoch [42/200] Batch [400/469] Loss D: 1.3912 Loss G: 1.1856\n",
            "Epoch [43/200] Batch [0/469] Loss D: 0.6345 Loss G: 1.5688\n",
            "Epoch [43/200] Batch [100/469] Loss D: 0.8408 Loss G: 1.7592\n",
            "Epoch [43/200] Batch [200/469] Loss D: 0.9528 Loss G: 1.4493\n",
            "Epoch [43/200] Batch [300/469] Loss D: 0.8460 Loss G: 1.0738\n",
            "Epoch [43/200] Batch [400/469] Loss D: 0.9309 Loss G: 1.6000\n",
            "Epoch [44/200] Batch [0/469] Loss D: 1.0518 Loss G: 1.5494\n",
            "Epoch [44/200] Batch [100/469] Loss D: 0.7075 Loss G: 1.2333\n",
            "Epoch [44/200] Batch [200/469] Loss D: 0.9038 Loss G: 1.2611\n",
            "Epoch [44/200] Batch [300/469] Loss D: 1.0604 Loss G: 1.3728\n",
            "Epoch [44/200] Batch [400/469] Loss D: 0.8545 Loss G: 1.4838\n",
            "Epoch [45/200] Batch [0/469] Loss D: 1.1981 Loss G: 1.3296\n",
            "Epoch [45/200] Batch [100/469] Loss D: 1.4932 Loss G: 0.9177\n",
            "Epoch [45/200] Batch [200/469] Loss D: 1.5174 Loss G: 0.9221\n",
            "Epoch [45/200] Batch [300/469] Loss D: 0.7733 Loss G: 1.4303\n",
            "Epoch [45/200] Batch [400/469] Loss D: 1.0018 Loss G: 1.6976\n",
            "Epoch [46/200] Batch [0/469] Loss D: 0.7384 Loss G: 1.5738\n",
            "Epoch [46/200] Batch [100/469] Loss D: 1.4978 Loss G: 1.0775\n",
            "Epoch [46/200] Batch [200/469] Loss D: 0.5427 Loss G: 1.9151\n",
            "Epoch [46/200] Batch [300/469] Loss D: 1.2751 Loss G: 1.0269\n",
            "Epoch [46/200] Batch [400/469] Loss D: 1.0359 Loss G: 1.0441\n",
            "Epoch [47/200] Batch [0/469] Loss D: 0.8795 Loss G: 1.7661\n",
            "Epoch [47/200] Batch [100/469] Loss D: 0.4485 Loss G: 1.7491\n",
            "Epoch [47/200] Batch [200/469] Loss D: 1.0597 Loss G: 1.7950\n",
            "Epoch [47/200] Batch [300/469] Loss D: 1.8276 Loss G: 0.7957\n",
            "Epoch [47/200] Batch [400/469] Loss D: 0.8523 Loss G: 1.3599\n",
            "Epoch [48/200] Batch [0/469] Loss D: 0.8183 Loss G: 1.2168\n",
            "Epoch [48/200] Batch [100/469] Loss D: 1.2046 Loss G: 0.9070\n",
            "Epoch [48/200] Batch [200/469] Loss D: 1.4168 Loss G: 0.9666\n",
            "Epoch [48/200] Batch [300/469] Loss D: 1.2590 Loss G: 1.3228\n",
            "Epoch [48/200] Batch [400/469] Loss D: 0.5575 Loss G: 1.4813\n",
            "Epoch [49/200] Batch [0/469] Loss D: 1.3700 Loss G: 0.9359\n",
            "Epoch [49/200] Batch [100/469] Loss D: 1.2504 Loss G: 1.1654\n",
            "Epoch [49/200] Batch [200/469] Loss D: 0.5078 Loss G: 1.8916\n",
            "Epoch [49/200] Batch [300/469] Loss D: 0.9669 Loss G: 1.3819\n",
            "Epoch [49/200] Batch [400/469] Loss D: 1.3212 Loss G: 1.1491\n",
            "Epoch [50/200] Batch [0/469] Loss D: 1.1736 Loss G: 1.4165\n",
            "Epoch [50/200] Batch [100/469] Loss D: 1.4465 Loss G: 1.8629\n",
            "Epoch [50/200] Batch [200/469] Loss D: 1.0995 Loss G: 1.5189\n",
            "Epoch [50/200] Batch [300/469] Loss D: 0.6736 Loss G: 1.9237\n",
            "Epoch [50/200] Batch [400/469] Loss D: 0.7583 Loss G: 1.4700\n",
            "Epoch [51/200] Batch [0/469] Loss D: 0.7400 Loss G: 1.5851\n",
            "Epoch [51/200] Batch [100/469] Loss D: 0.6347 Loss G: 1.9535\n",
            "Epoch [51/200] Batch [200/469] Loss D: 1.2501 Loss G: 1.0595\n",
            "Epoch [51/200] Batch [300/469] Loss D: 0.4205 Loss G: 1.6768\n",
            "Epoch [51/200] Batch [400/469] Loss D: 0.9185 Loss G: 1.4179\n",
            "Epoch [52/200] Batch [0/469] Loss D: 0.7152 Loss G: 1.5376\n",
            "Epoch [52/200] Batch [100/469] Loss D: 0.8677 Loss G: 1.4743\n",
            "Epoch [52/200] Batch [200/469] Loss D: 0.7372 Loss G: 1.4830\n",
            "Epoch [52/200] Batch [300/469] Loss D: 1.5134 Loss G: 1.2756\n",
            "Epoch [52/200] Batch [400/469] Loss D: 0.6413 Loss G: 1.4014\n",
            "Epoch [53/200] Batch [0/469] Loss D: 1.3068 Loss G: 1.9606\n",
            "Epoch [53/200] Batch [100/469] Loss D: 1.0807 Loss G: 1.7162\n",
            "Epoch [53/200] Batch [200/469] Loss D: 0.9849 Loss G: 1.7342\n",
            "Epoch [53/200] Batch [300/469] Loss D: 0.7435 Loss G: 1.5182\n",
            "Epoch [53/200] Batch [400/469] Loss D: 1.0862 Loss G: 1.0262\n",
            "Epoch [54/200] Batch [0/469] Loss D: 1.4113 Loss G: 1.2690\n",
            "Epoch [54/200] Batch [100/469] Loss D: 1.0932 Loss G: 0.9510\n",
            "Epoch [54/200] Batch [200/469] Loss D: 0.8344 Loss G: 1.4532\n",
            "Epoch [54/200] Batch [300/469] Loss D: 0.5822 Loss G: 1.5909\n",
            "Epoch [54/200] Batch [400/469] Loss D: 0.6817 Loss G: 1.4841\n",
            "Epoch [55/200] Batch [0/469] Loss D: 0.4623 Loss G: 2.0483\n",
            "Epoch [55/200] Batch [100/469] Loss D: 1.1777 Loss G: 0.9897\n",
            "Epoch [55/200] Batch [200/469] Loss D: 0.7386 Loss G: 1.6678\n",
            "Epoch [55/200] Batch [300/469] Loss D: 0.6967 Loss G: 1.5891\n",
            "Epoch [55/200] Batch [400/469] Loss D: 0.7277 Loss G: 1.7344\n",
            "Epoch [56/200] Batch [0/469] Loss D: 1.0936 Loss G: 1.8338\n",
            "Epoch [56/200] Batch [100/469] Loss D: 0.9162 Loss G: 1.5592\n",
            "Epoch [56/200] Batch [200/469] Loss D: 0.6991 Loss G: 1.7551\n",
            "Epoch [56/200] Batch [300/469] Loss D: 0.8089 Loss G: 1.5267\n",
            "Epoch [56/200] Batch [400/469] Loss D: 0.9138 Loss G: 1.2661\n",
            "Epoch [57/200] Batch [0/469] Loss D: 0.6740 Loss G: 1.8932\n",
            "Epoch [57/200] Batch [100/469] Loss D: 0.5888 Loss G: 1.7661\n",
            "Epoch [57/200] Batch [200/469] Loss D: 1.1478 Loss G: 1.5062\n",
            "Epoch [57/200] Batch [300/469] Loss D: 0.5832 Loss G: 1.7533\n",
            "Epoch [57/200] Batch [400/469] Loss D: 0.7608 Loss G: 1.5973\n",
            "Epoch [58/200] Batch [0/469] Loss D: 1.3241 Loss G: 1.4064\n",
            "Epoch [58/200] Batch [100/469] Loss D: 0.7668 Loss G: 1.8105\n",
            "Epoch [58/200] Batch [200/469] Loss D: 0.9101 Loss G: 1.9160\n",
            "Epoch [58/200] Batch [300/469] Loss D: 0.9171 Loss G: 1.3780\n",
            "Epoch [58/200] Batch [400/469] Loss D: 0.9381 Loss G: 1.7878\n",
            "Epoch [59/200] Batch [0/469] Loss D: 0.7444 Loss G: 2.0584\n",
            "Epoch [59/200] Batch [100/469] Loss D: 0.9273 Loss G: 2.1870\n",
            "Epoch [59/200] Batch [200/469] Loss D: 0.7216 Loss G: 1.4310\n",
            "Epoch [59/200] Batch [300/469] Loss D: 0.6138 Loss G: 1.9345\n",
            "Epoch [59/200] Batch [400/469] Loss D: 0.6064 Loss G: 1.7087\n",
            "Epoch [60/200] Batch [0/469] Loss D: 0.7355 Loss G: 1.8746\n",
            "Epoch [60/200] Batch [100/469] Loss D: 0.9322 Loss G: 1.2023\n",
            "Epoch [60/200] Batch [200/469] Loss D: 1.3001 Loss G: 1.1177\n",
            "Epoch [60/200] Batch [300/469] Loss D: 0.6660 Loss G: 2.0507\n",
            "Epoch [60/200] Batch [400/469] Loss D: 0.9579 Loss G: 1.3424\n",
            "Epoch [61/200] Batch [0/469] Loss D: 1.2309 Loss G: 1.7326\n",
            "Epoch [61/200] Batch [100/469] Loss D: 0.7227 Loss G: 1.1248\n",
            "Epoch [61/200] Batch [200/469] Loss D: 0.5178 Loss G: 1.8740\n",
            "Epoch [61/200] Batch [300/469] Loss D: 0.7926 Loss G: 2.0135\n",
            "Epoch [61/200] Batch [400/469] Loss D: 1.0578 Loss G: 1.1392\n",
            "Epoch [62/200] Batch [0/469] Loss D: 0.6730 Loss G: 1.6138\n",
            "Epoch [62/200] Batch [100/469] Loss D: 0.7507 Loss G: 1.5999\n",
            "Epoch [62/200] Batch [200/469] Loss D: 1.1576 Loss G: 1.3588\n",
            "Epoch [62/200] Batch [300/469] Loss D: 0.9579 Loss G: 1.4617\n",
            "Epoch [62/200] Batch [400/469] Loss D: 1.1495 Loss G: 1.0757\n",
            "Epoch [63/200] Batch [0/469] Loss D: 0.7980 Loss G: 2.2670\n",
            "Epoch [63/200] Batch [100/469] Loss D: 0.9025 Loss G: 1.4831\n",
            "Epoch [63/200] Batch [200/469] Loss D: 0.4699 Loss G: 2.0192\n",
            "Epoch [63/200] Batch [300/469] Loss D: 0.4320 Loss G: 2.0726\n",
            "Epoch [63/200] Batch [400/469] Loss D: 0.7877 Loss G: 1.5266\n",
            "Epoch [64/200] Batch [0/469] Loss D: 0.6881 Loss G: 1.5081\n",
            "Epoch [64/200] Batch [100/469] Loss D: 0.4058 Loss G: 2.6419\n",
            "Epoch [64/200] Batch [200/469] Loss D: 1.1437 Loss G: 1.7195\n",
            "Epoch [64/200] Batch [300/469] Loss D: 0.6056 Loss G: 2.2482\n",
            "Epoch [64/200] Batch [400/469] Loss D: 1.0259 Loss G: 2.1602\n",
            "Epoch [65/200] Batch [0/469] Loss D: 0.4623 Loss G: 2.1023\n",
            "Epoch [65/200] Batch [100/469] Loss D: 0.8183 Loss G: 1.7218\n",
            "Epoch [65/200] Batch [200/469] Loss D: 0.7508 Loss G: 1.7519\n",
            "Epoch [65/200] Batch [300/469] Loss D: 0.4822 Loss G: 2.0408\n",
            "Epoch [65/200] Batch [400/469] Loss D: 0.6740 Loss G: 1.6041\n",
            "Epoch [66/200] Batch [0/469] Loss D: 0.8594 Loss G: 1.7459\n",
            "Epoch [66/200] Batch [100/469] Loss D: 0.7933 Loss G: 1.2903\n",
            "Epoch [66/200] Batch [200/469] Loss D: 0.8129 Loss G: 1.2842\n",
            "Epoch [66/200] Batch [300/469] Loss D: 1.4697 Loss G: 1.0731\n",
            "Epoch [66/200] Batch [400/469] Loss D: 0.8472 Loss G: 1.7780\n",
            "Epoch [67/200] Batch [0/469] Loss D: 0.7156 Loss G: 1.9644\n",
            "Epoch [67/200] Batch [100/469] Loss D: 0.5528 Loss G: 1.6828\n",
            "Epoch [67/200] Batch [200/469] Loss D: 1.1744 Loss G: 1.1197\n",
            "Epoch [67/200] Batch [300/469] Loss D: 0.7329 Loss G: 2.2055\n",
            "Epoch [67/200] Batch [400/469] Loss D: 0.5236 Loss G: 2.1233\n",
            "Epoch [68/200] Batch [0/469] Loss D: 0.7054 Loss G: 2.3543\n",
            "Epoch [68/200] Batch [100/469] Loss D: 0.5737 Loss G: 2.0900\n",
            "Epoch [68/200] Batch [200/469] Loss D: 0.4029 Loss G: 2.1675\n",
            "Epoch [68/200] Batch [300/469] Loss D: 0.8093 Loss G: 2.1814\n",
            "Epoch [68/200] Batch [400/469] Loss D: 0.9242 Loss G: 1.4059\n",
            "Epoch [69/200] Batch [0/469] Loss D: 0.7869 Loss G: 1.5421\n",
            "Epoch [69/200] Batch [100/469] Loss D: 0.7059 Loss G: 1.9737\n",
            "Epoch [69/200] Batch [200/469] Loss D: 0.5951 Loss G: 1.8948\n",
            "Epoch [69/200] Batch [300/469] Loss D: 0.8878 Loss G: 2.1522\n",
            "Epoch [69/200] Batch [400/469] Loss D: 0.6922 Loss G: 2.0496\n",
            "Epoch [70/200] Batch [0/469] Loss D: 0.9251 Loss G: 2.6561\n",
            "Epoch [70/200] Batch [100/469] Loss D: 1.1259 Loss G: 2.0779\n",
            "Epoch [70/200] Batch [200/469] Loss D: 0.7109 Loss G: 1.9598\n",
            "Epoch [70/200] Batch [300/469] Loss D: 0.9066 Loss G: 1.7577\n",
            "Epoch [70/200] Batch [400/469] Loss D: 1.1560 Loss G: 2.5813\n",
            "Epoch [71/200] Batch [0/469] Loss D: 1.4529 Loss G: 1.9996\n",
            "Epoch [71/200] Batch [100/469] Loss D: 0.7288 Loss G: 1.8576\n",
            "Epoch [71/200] Batch [200/469] Loss D: 0.4838 Loss G: 2.3108\n",
            "Epoch [71/200] Batch [300/469] Loss D: 0.6891 Loss G: 1.7817\n",
            "Epoch [71/200] Batch [400/469] Loss D: 0.5489 Loss G: 2.6287\n",
            "Epoch [72/200] Batch [0/469] Loss D: 1.0893 Loss G: 1.8883\n",
            "Epoch [72/200] Batch [100/469] Loss D: 0.6616 Loss G: 2.0626\n",
            "Epoch [72/200] Batch [200/469] Loss D: 0.3958 Loss G: 1.9949\n",
            "Epoch [72/200] Batch [300/469] Loss D: 0.6930 Loss G: 1.6969\n",
            "Epoch [72/200] Batch [400/469] Loss D: 0.7780 Loss G: 1.8606\n",
            "Epoch [73/200] Batch [0/469] Loss D: 1.6099 Loss G: 2.2844\n",
            "Epoch [73/200] Batch [100/469] Loss D: 0.7146 Loss G: 1.6024\n",
            "Epoch [73/200] Batch [200/469] Loss D: 0.6464 Loss G: 2.0420\n",
            "Epoch [73/200] Batch [300/469] Loss D: 0.8821 Loss G: 2.0239\n",
            "Epoch [73/200] Batch [400/469] Loss D: 0.4702 Loss G: 2.4142\n",
            "Epoch [74/200] Batch [0/469] Loss D: 0.6173 Loss G: 2.0808\n",
            "Epoch [74/200] Batch [100/469] Loss D: 0.7873 Loss G: 1.4499\n",
            "Epoch [74/200] Batch [200/469] Loss D: 0.6273 Loss G: 2.0042\n",
            "Epoch [74/200] Batch [300/469] Loss D: 0.7638 Loss G: 1.9298\n",
            "Epoch [74/200] Batch [400/469] Loss D: 0.5511 Loss G: 2.2882\n",
            "Epoch [75/200] Batch [0/469] Loss D: 0.8196 Loss G: 1.8424\n",
            "Epoch [75/200] Batch [100/469] Loss D: 0.9789 Loss G: 2.4925\n",
            "Epoch [75/200] Batch [200/469] Loss D: 1.0445 Loss G: 1.9274\n",
            "Epoch [75/200] Batch [300/469] Loss D: 0.6327 Loss G: 2.3775\n",
            "Epoch [75/200] Batch [400/469] Loss D: 0.5725 Loss G: 1.5821\n",
            "Epoch [76/200] Batch [0/469] Loss D: 0.7934 Loss G: 1.8702\n",
            "Epoch [76/200] Batch [100/469] Loss D: 0.4775 Loss G: 1.8433\n",
            "Epoch [76/200] Batch [200/469] Loss D: 0.5760 Loss G: 2.8928\n",
            "Epoch [76/200] Batch [300/469] Loss D: 0.3986 Loss G: 2.7061\n",
            "Epoch [76/200] Batch [400/469] Loss D: 0.3106 Loss G: 2.0716\n",
            "Epoch [77/200] Batch [0/469] Loss D: 0.7801 Loss G: 2.4799\n",
            "Epoch [77/200] Batch [100/469] Loss D: 0.4145 Loss G: 2.4551\n",
            "Epoch [77/200] Batch [200/469] Loss D: 0.7077 Loss G: 1.9305\n",
            "Epoch [77/200] Batch [300/469] Loss D: 0.7628 Loss G: 2.2108\n",
            "Epoch [77/200] Batch [400/469] Loss D: 0.7251 Loss G: 1.9316\n",
            "Epoch [78/200] Batch [0/469] Loss D: 0.4571 Loss G: 1.8776\n",
            "Epoch [78/200] Batch [100/469] Loss D: 0.6138 Loss G: 2.0543\n",
            "Epoch [78/200] Batch [200/469] Loss D: 0.5095 Loss G: 2.2617\n",
            "Epoch [78/200] Batch [300/469] Loss D: 0.5278 Loss G: 2.2818\n",
            "Epoch [78/200] Batch [400/469] Loss D: 0.7636 Loss G: 1.6500\n",
            "Epoch [79/200] Batch [0/469] Loss D: 0.7081 Loss G: 1.9132\n",
            "Epoch [79/200] Batch [100/469] Loss D: 0.6455 Loss G: 1.4790\n",
            "Epoch [79/200] Batch [200/469] Loss D: 0.5893 Loss G: 2.4157\n",
            "Epoch [79/200] Batch [300/469] Loss D: 0.4721 Loss G: 2.6129\n",
            "Epoch [79/200] Batch [400/469] Loss D: 0.4734 Loss G: 2.6759\n",
            "Epoch [80/200] Batch [0/469] Loss D: 0.5207 Loss G: 2.2312\n",
            "Epoch [80/200] Batch [100/469] Loss D: 0.5110 Loss G: 2.1948\n",
            "Epoch [80/200] Batch [200/469] Loss D: 0.5879 Loss G: 2.2210\n",
            "Epoch [80/200] Batch [300/469] Loss D: 0.3201 Loss G: 2.5172\n",
            "Epoch [80/200] Batch [400/469] Loss D: 1.2301 Loss G: 2.7355\n",
            "Epoch [81/200] Batch [0/469] Loss D: 0.4031 Loss G: 2.3750\n",
            "Epoch [81/200] Batch [100/469] Loss D: 0.4129 Loss G: 1.5270\n",
            "Epoch [81/200] Batch [200/469] Loss D: 0.8194 Loss G: 3.1813\n",
            "Epoch [81/200] Batch [300/469] Loss D: 0.4705 Loss G: 2.3305\n",
            "Epoch [81/200] Batch [400/469] Loss D: 0.5284 Loss G: 2.1501\n",
            "Epoch [82/200] Batch [0/469] Loss D: 0.7079 Loss G: 2.8888\n",
            "Epoch [82/200] Batch [100/469] Loss D: 0.3691 Loss G: 2.9832\n",
            "Epoch [82/200] Batch [200/469] Loss D: 0.4590 Loss G: 3.2145\n",
            "Epoch [82/200] Batch [300/469] Loss D: 0.3447 Loss G: 2.5467\n",
            "Epoch [82/200] Batch [400/469] Loss D: 0.4949 Loss G: 1.5652\n",
            "Epoch [83/200] Batch [0/469] Loss D: 0.8114 Loss G: 2.3902\n",
            "Epoch [83/200] Batch [100/469] Loss D: 0.6678 Loss G: 2.3409\n",
            "Epoch [83/200] Batch [200/469] Loss D: 0.8735 Loss G: 1.9275\n",
            "Epoch [83/200] Batch [300/469] Loss D: 0.5787 Loss G: 2.1730\n",
            "Epoch [83/200] Batch [400/469] Loss D: 0.5881 Loss G: 2.5276\n",
            "Epoch [84/200] Batch [0/469] Loss D: 1.0225 Loss G: 2.5680\n",
            "Epoch [84/200] Batch [100/469] Loss D: 0.3821 Loss G: 1.7419\n",
            "Epoch [84/200] Batch [200/469] Loss D: 0.4393 Loss G: 1.8143\n",
            "Epoch [84/200] Batch [300/469] Loss D: 0.7421 Loss G: 1.4428\n",
            "Epoch [84/200] Batch [400/469] Loss D: 0.5913 Loss G: 2.4593\n",
            "Epoch [85/200] Batch [0/469] Loss D: 0.3840 Loss G: 3.1491\n",
            "Epoch [85/200] Batch [100/469] Loss D: 0.6273 Loss G: 1.6675\n",
            "Epoch [85/200] Batch [200/469] Loss D: 0.9214 Loss G: 1.2007\n",
            "Epoch [85/200] Batch [300/469] Loss D: 0.7979 Loss G: 2.7342\n",
            "Epoch [85/200] Batch [400/469] Loss D: 0.4122 Loss G: 2.8297\n",
            "Epoch [86/200] Batch [0/469] Loss D: 0.6879 Loss G: 1.7885\n",
            "Epoch [86/200] Batch [100/469] Loss D: 0.4117 Loss G: 2.5501\n",
            "Epoch [86/200] Batch [200/469] Loss D: 1.0623 Loss G: 3.1275\n",
            "Epoch [86/200] Batch [300/469] Loss D: 0.3880 Loss G: 2.8306\n",
            "Epoch [86/200] Batch [400/469] Loss D: 0.6283 Loss G: 2.5247\n",
            "Epoch [87/200] Batch [0/469] Loss D: 0.3743 Loss G: 2.3202\n",
            "Epoch [87/200] Batch [100/469] Loss D: 0.5572 Loss G: 1.9886\n",
            "Epoch [87/200] Batch [200/469] Loss D: 0.5470 Loss G: 2.4811\n",
            "Epoch [87/200] Batch [300/469] Loss D: 0.8558 Loss G: 3.2512\n",
            "Epoch [87/200] Batch [400/469] Loss D: 0.9196 Loss G: 2.4720\n",
            "Epoch [88/200] Batch [0/469] Loss D: 0.4727 Loss G: 2.7165\n",
            "Epoch [88/200] Batch [100/469] Loss D: 0.3854 Loss G: 2.6717\n",
            "Epoch [88/200] Batch [200/469] Loss D: 0.4365 Loss G: 2.4744\n",
            "Epoch [88/200] Batch [300/469] Loss D: 0.5798 Loss G: 2.8781\n",
            "Epoch [88/200] Batch [400/469] Loss D: 0.3908 Loss G: 2.1285\n",
            "Epoch [89/200] Batch [0/469] Loss D: 0.4376 Loss G: 2.3635\n",
            "Epoch [89/200] Batch [100/469] Loss D: 0.5642 Loss G: 2.2098\n",
            "Epoch [89/200] Batch [200/469] Loss D: 0.4414 Loss G: 2.5679\n",
            "Epoch [89/200] Batch [300/469] Loss D: 0.4610 Loss G: 2.5129\n",
            "Epoch [89/200] Batch [400/469] Loss D: 0.5751 Loss G: 2.4505\n",
            "Epoch [90/200] Batch [0/469] Loss D: 0.4705 Loss G: 2.7397\n",
            "Epoch [90/200] Batch [100/469] Loss D: 0.5061 Loss G: 2.2300\n",
            "Epoch [90/200] Batch [200/469] Loss D: 0.7581 Loss G: 2.8538\n",
            "Epoch [90/200] Batch [300/469] Loss D: 0.6960 Loss G: 1.8124\n",
            "Epoch [90/200] Batch [400/469] Loss D: 0.5171 Loss G: 2.0266\n",
            "Epoch [91/200] Batch [0/469] Loss D: 0.6745 Loss G: 1.5796\n",
            "Epoch [91/200] Batch [100/469] Loss D: 0.8115 Loss G: 2.2118\n",
            "Epoch [91/200] Batch [200/469] Loss D: 0.3168 Loss G: 3.6459\n",
            "Epoch [91/200] Batch [300/469] Loss D: 0.5442 Loss G: 2.8356\n",
            "Epoch [91/200] Batch [400/469] Loss D: 0.5103 Loss G: 3.2508\n",
            "Epoch [92/200] Batch [0/469] Loss D: 1.2832 Loss G: 3.6752\n",
            "Epoch [92/200] Batch [100/469] Loss D: 0.5811 Loss G: 2.4706\n",
            "Epoch [92/200] Batch [200/469] Loss D: 0.5990 Loss G: 2.1388\n",
            "Epoch [92/200] Batch [300/469] Loss D: 0.4727 Loss G: 2.5519\n",
            "Epoch [92/200] Batch [400/469] Loss D: 0.7041 Loss G: 2.4419\n",
            "Epoch [93/200] Batch [0/469] Loss D: 0.3957 Loss G: 2.4470\n",
            "Epoch [93/200] Batch [100/469] Loss D: 0.5006 Loss G: 3.5156\n",
            "Epoch [93/200] Batch [200/469] Loss D: 0.2701 Loss G: 2.5127\n",
            "Epoch [93/200] Batch [300/469] Loss D: 0.4992 Loss G: 2.0115\n",
            "Epoch [93/200] Batch [400/469] Loss D: 0.4403 Loss G: 3.1751\n",
            "Epoch [94/200] Batch [0/469] Loss D: 0.4905 Loss G: 2.1352\n",
            "Epoch [94/200] Batch [100/469] Loss D: 0.6712 Loss G: 1.3396\n",
            "Epoch [94/200] Batch [200/469] Loss D: 0.5044 Loss G: 1.9061\n",
            "Epoch [94/200] Batch [300/469] Loss D: 0.4783 Loss G: 2.9499\n",
            "Epoch [94/200] Batch [400/469] Loss D: 0.4240 Loss G: 2.6466\n",
            "Epoch [95/200] Batch [0/469] Loss D: 0.3359 Loss G: 3.0234\n",
            "Epoch [95/200] Batch [100/469] Loss D: 0.2289 Loss G: 3.8968\n",
            "Epoch [95/200] Batch [200/469] Loss D: 0.7406 Loss G: 2.1814\n",
            "Epoch [95/200] Batch [300/469] Loss D: 0.5537 Loss G: 2.2507\n",
            "Epoch [95/200] Batch [400/469] Loss D: 0.4536 Loss G: 2.6359\n",
            "Epoch [96/200] Batch [0/469] Loss D: 0.4421 Loss G: 4.0449\n",
            "Epoch [96/200] Batch [100/469] Loss D: 0.8221 Loss G: 1.6728\n",
            "Epoch [96/200] Batch [200/469] Loss D: 0.2549 Loss G: 2.6882\n",
            "Epoch [96/200] Batch [300/469] Loss D: 0.3363 Loss G: 3.0185\n",
            "Epoch [96/200] Batch [400/469] Loss D: 0.4265 Loss G: 2.4306\n",
            "Epoch [97/200] Batch [0/469] Loss D: 0.7982 Loss G: 3.3884\n",
            "Epoch [97/200] Batch [100/469] Loss D: 0.6306 Loss G: 1.9551\n",
            "Epoch [97/200] Batch [200/469] Loss D: 1.3720 Loss G: 3.8092\n",
            "Epoch [97/200] Batch [300/469] Loss D: 1.0067 Loss G: 1.6749\n",
            "Epoch [97/200] Batch [400/469] Loss D: 0.3495 Loss G: 2.7591\n",
            "Epoch [98/200] Batch [0/469] Loss D: 0.5387 Loss G: 3.0212\n",
            "Epoch [98/200] Batch [100/469] Loss D: 0.5573 Loss G: 2.1261\n",
            "Epoch [98/200] Batch [200/469] Loss D: 0.3466 Loss G: 2.9818\n",
            "Epoch [98/200] Batch [300/469] Loss D: 0.3977 Loss G: 3.5133\n",
            "Epoch [98/200] Batch [400/469] Loss D: 0.3140 Loss G: 2.2234\n",
            "Epoch [99/200] Batch [0/469] Loss D: 0.4239 Loss G: 3.3999\n",
            "Epoch [99/200] Batch [100/469] Loss D: 0.5546 Loss G: 2.5188\n",
            "Epoch [99/200] Batch [200/469] Loss D: 0.6916 Loss G: 3.7931\n",
            "Epoch [99/200] Batch [300/469] Loss D: 0.7977 Loss G: 1.5321\n",
            "Epoch [99/200] Batch [400/469] Loss D: 0.4025 Loss G: 3.3151\n",
            "Epoch [100/200] Batch [0/469] Loss D: 0.4586 Loss G: 4.0592\n",
            "Epoch [100/200] Batch [100/469] Loss D: 1.3481 Loss G: 3.6855\n",
            "Epoch [100/200] Batch [200/469] Loss D: 0.2485 Loss G: 3.1697\n",
            "Epoch [100/200] Batch [300/469] Loss D: 0.5008 Loss G: 2.4806\n",
            "Epoch [100/200] Batch [400/469] Loss D: 0.5363 Loss G: 3.9481\n",
            "Epoch [101/200] Batch [0/469] Loss D: 0.5765 Loss G: 3.0982\n",
            "Epoch [101/200] Batch [100/469] Loss D: 0.4518 Loss G: 2.8459\n",
            "Epoch [101/200] Batch [200/469] Loss D: 0.4209 Loss G: 2.4500\n",
            "Epoch [101/200] Batch [300/469] Loss D: 0.9213 Loss G: 2.6221\n",
            "Epoch [101/200] Batch [400/469] Loss D: 0.4029 Loss G: 2.7872\n",
            "Epoch [102/200] Batch [0/469] Loss D: 0.4137 Loss G: 3.4806\n",
            "Epoch [102/200] Batch [100/469] Loss D: 0.4258 Loss G: 2.2370\n",
            "Epoch [102/200] Batch [200/469] Loss D: 0.4000 Loss G: 2.8688\n",
            "Epoch [102/200] Batch [300/469] Loss D: 0.4154 Loss G: 2.4920\n",
            "Epoch [102/200] Batch [400/469] Loss D: 0.3655 Loss G: 3.1604\n",
            "Epoch [103/200] Batch [0/469] Loss D: 0.5050 Loss G: 2.3993\n",
            "Epoch [103/200] Batch [100/469] Loss D: 0.3206 Loss G: 2.9465\n",
            "Epoch [103/200] Batch [200/469] Loss D: 0.3892 Loss G: 2.9905\n",
            "Epoch [103/200] Batch [300/469] Loss D: 0.3245 Loss G: 2.4614\n",
            "Epoch [103/200] Batch [400/469] Loss D: 0.3652 Loss G: 2.7775\n",
            "Epoch [104/200] Batch [0/469] Loss D: 0.5059 Loss G: 3.5981\n",
            "Epoch [104/200] Batch [100/469] Loss D: 0.2074 Loss G: 3.4405\n",
            "Epoch [104/200] Batch [200/469] Loss D: 0.7311 Loss G: 3.2950\n",
            "Epoch [104/200] Batch [300/469] Loss D: 0.3560 Loss G: 2.1205\n",
            "Epoch [104/200] Batch [400/469] Loss D: 0.4469 Loss G: 2.0605\n",
            "Epoch [105/200] Batch [0/469] Loss D: 0.5568 Loss G: 1.8863\n",
            "Epoch [105/200] Batch [100/469] Loss D: 0.4980 Loss G: 3.1248\n",
            "Epoch [105/200] Batch [200/469] Loss D: 0.4133 Loss G: 2.1653\n",
            "Epoch [105/200] Batch [300/469] Loss D: 0.8150 Loss G: 1.0856\n",
            "Epoch [105/200] Batch [400/469] Loss D: 0.3811 Loss G: 4.1380\n",
            "Epoch [106/200] Batch [0/469] Loss D: 0.3492 Loss G: 2.4696\n",
            "Epoch [106/200] Batch [100/469] Loss D: 0.2052 Loss G: 3.2800\n",
            "Epoch [106/200] Batch [200/469] Loss D: 0.3319 Loss G: 2.5529\n",
            "Epoch [106/200] Batch [300/469] Loss D: 0.4095 Loss G: 3.2677\n",
            "Epoch [106/200] Batch [400/469] Loss D: 0.4419 Loss G: 2.4643\n",
            "Epoch [107/200] Batch [0/469] Loss D: 0.4487 Loss G: 2.6026\n",
            "Epoch [107/200] Batch [100/469] Loss D: 0.3322 Loss G: 3.2525\n",
            "Epoch [107/200] Batch [200/469] Loss D: 0.5358 Loss G: 3.1785\n",
            "Epoch [107/200] Batch [300/469] Loss D: 0.3688 Loss G: 2.9666\n",
            "Epoch [107/200] Batch [400/469] Loss D: 0.3349 Loss G: 2.8098\n",
            "Epoch [108/200] Batch [0/469] Loss D: 0.5246 Loss G: 3.6214\n",
            "Epoch [108/200] Batch [100/469] Loss D: 0.2954 Loss G: 2.7546\n",
            "Epoch [108/200] Batch [200/469] Loss D: 0.4520 Loss G: 2.0324\n",
            "Epoch [108/200] Batch [300/469] Loss D: 0.4767 Loss G: 2.0605\n",
            "Epoch [108/200] Batch [400/469] Loss D: 0.1890 Loss G: 3.9168\n",
            "Epoch [109/200] Batch [0/469] Loss D: 0.2491 Loss G: 2.6985\n",
            "Epoch [109/200] Batch [100/469] Loss D: 0.3190 Loss G: 2.9159\n",
            "Epoch [109/200] Batch [200/469] Loss D: 0.2130 Loss G: 4.3936\n",
            "Epoch [109/200] Batch [300/469] Loss D: 0.2531 Loss G: 2.9660\n",
            "Epoch [109/200] Batch [400/469] Loss D: 0.4039 Loss G: 3.7664\n",
            "Epoch [110/200] Batch [0/469] Loss D: 0.3405 Loss G: 3.3877\n",
            "Epoch [110/200] Batch [100/469] Loss D: 0.3425 Loss G: 2.7598\n",
            "Epoch [110/200] Batch [200/469] Loss D: 0.2234 Loss G: 3.5303\n",
            "Epoch [110/200] Batch [300/469] Loss D: 0.2515 Loss G: 3.7697\n",
            "Epoch [110/200] Batch [400/469] Loss D: 0.5029 Loss G: 2.7352\n",
            "Epoch [111/200] Batch [0/469] Loss D: 0.4548 Loss G: 2.3396\n",
            "Epoch [111/200] Batch [100/469] Loss D: 0.3008 Loss G: 3.0629\n",
            "Epoch [111/200] Batch [200/469] Loss D: 0.2619 Loss G: 3.5782\n",
            "Epoch [111/200] Batch [300/469] Loss D: 0.2115 Loss G: 4.0651\n",
            "Epoch [111/200] Batch [400/469] Loss D: 0.3245 Loss G: 3.7788\n",
            "Epoch [112/200] Batch [0/469] Loss D: 0.4418 Loss G: 3.3115\n",
            "Epoch [112/200] Batch [100/469] Loss D: 0.2694 Loss G: 3.8201\n",
            "Epoch [112/200] Batch [200/469] Loss D: 0.4033 Loss G: 3.0377\n",
            "Epoch [112/200] Batch [300/469] Loss D: 0.2279 Loss G: 3.5698\n",
            "Epoch [112/200] Batch [400/469] Loss D: 0.4196 Loss G: 3.5117\n",
            "Epoch [113/200] Batch [0/469] Loss D: 0.4865 Loss G: 3.5535\n",
            "Epoch [113/200] Batch [100/469] Loss D: 0.4903 Loss G: 3.1748\n",
            "Epoch [113/200] Batch [200/469] Loss D: 1.0431 Loss G: 1.1831\n",
            "Epoch [113/200] Batch [300/469] Loss D: 0.1731 Loss G: 3.6369\n",
            "Epoch [113/200] Batch [400/469] Loss D: 0.9559 Loss G: 1.0921\n",
            "Epoch [114/200] Batch [0/469] Loss D: 0.4948 Loss G: 2.1526\n",
            "Epoch [114/200] Batch [100/469] Loss D: 0.3784 Loss G: 2.2661\n",
            "Epoch [114/200] Batch [200/469] Loss D: 0.7299 Loss G: 3.1718\n",
            "Epoch [114/200] Batch [300/469] Loss D: 0.3620 Loss G: 2.4795\n",
            "Epoch [114/200] Batch [400/469] Loss D: 0.6221 Loss G: 1.9608\n",
            "Epoch [115/200] Batch [0/469] Loss D: 0.6108 Loss G: 4.0821\n",
            "Epoch [115/200] Batch [100/469] Loss D: 0.4571 Loss G: 4.2684\n",
            "Epoch [115/200] Batch [200/469] Loss D: 0.3504 Loss G: 3.8005\n",
            "Epoch [115/200] Batch [300/469] Loss D: 0.5809 Loss G: 3.5070\n",
            "Epoch [115/200] Batch [400/469] Loss D: 0.5917 Loss G: 4.5245\n",
            "Epoch [116/200] Batch [0/469] Loss D: 0.1899 Loss G: 3.3062\n",
            "Epoch [116/200] Batch [100/469] Loss D: 0.2500 Loss G: 3.4562\n",
            "Epoch [116/200] Batch [200/469] Loss D: 0.3478 Loss G: 3.0429\n",
            "Epoch [116/200] Batch [300/469] Loss D: 0.2549 Loss G: 3.6905\n",
            "Epoch [116/200] Batch [400/469] Loss D: 0.2362 Loss G: 3.0979\n",
            "Epoch [117/200] Batch [0/469] Loss D: 0.5705 Loss G: 2.1227\n",
            "Epoch [117/200] Batch [100/469] Loss D: 0.3677 Loss G: 4.5720\n",
            "Epoch [117/200] Batch [200/469] Loss D: 0.2748 Loss G: 3.3879\n",
            "Epoch [117/200] Batch [300/469] Loss D: 0.1509 Loss G: 3.7570\n",
            "Epoch [117/200] Batch [400/469] Loss D: 0.4157 Loss G: 2.8975\n",
            "Epoch [118/200] Batch [0/469] Loss D: 0.1596 Loss G: 3.4270\n",
            "Epoch [118/200] Batch [100/469] Loss D: 0.4360 Loss G: 2.5215\n",
            "Epoch [118/200] Batch [200/469] Loss D: 0.3147 Loss G: 3.7903\n",
            "Epoch [118/200] Batch [300/469] Loss D: 0.2624 Loss G: 3.9810\n",
            "Epoch [118/200] Batch [400/469] Loss D: 0.1796 Loss G: 3.6647\n",
            "Epoch [119/200] Batch [0/469] Loss D: 0.3264 Loss G: 3.1183\n",
            "Epoch [119/200] Batch [100/469] Loss D: 0.7625 Loss G: 4.0162\n",
            "Epoch [119/200] Batch [200/469] Loss D: 0.5657 Loss G: 1.4456\n",
            "Epoch [119/200] Batch [300/469] Loss D: 0.1924 Loss G: 3.7982\n",
            "Epoch [119/200] Batch [400/469] Loss D: 0.3978 Loss G: 3.2200\n",
            "Epoch [120/200] Batch [0/469] Loss D: 0.3656 Loss G: 3.6302\n",
            "Epoch [120/200] Batch [100/469] Loss D: 0.2019 Loss G: 3.5456\n",
            "Epoch [120/200] Batch [200/469] Loss D: 0.1450 Loss G: 3.7025\n",
            "Epoch [120/200] Batch [300/469] Loss D: 0.2293 Loss G: 3.4912\n",
            "Epoch [120/200] Batch [400/469] Loss D: 0.2311 Loss G: 3.3641\n",
            "Epoch [121/200] Batch [0/469] Loss D: 0.9570 Loss G: 5.0134\n",
            "Epoch [121/200] Batch [100/469] Loss D: 0.2605 Loss G: 3.2586\n",
            "Epoch [121/200] Batch [200/469] Loss D: 0.2913 Loss G: 4.0546\n",
            "Epoch [121/200] Batch [300/469] Loss D: 0.3809 Loss G: 2.5082\n",
            "Epoch [121/200] Batch [400/469] Loss D: 0.1893 Loss G: 3.7920\n",
            "Epoch [122/200] Batch [0/469] Loss D: 0.3369 Loss G: 3.2362\n",
            "Epoch [122/200] Batch [100/469] Loss D: 0.2879 Loss G: 3.3068\n",
            "Epoch [122/200] Batch [200/469] Loss D: 0.3188 Loss G: 3.2135\n",
            "Epoch [122/200] Batch [300/469] Loss D: 0.3660 Loss G: 4.9274\n",
            "Epoch [122/200] Batch [400/469] Loss D: 0.1350 Loss G: 3.7791\n",
            "Epoch [123/200] Batch [0/469] Loss D: 0.2999 Loss G: 3.0374\n",
            "Epoch [123/200] Batch [100/469] Loss D: 0.3129 Loss G: 2.3462\n",
            "Epoch [123/200] Batch [200/469] Loss D: 0.3954 Loss G: 3.1767\n",
            "Epoch [123/200] Batch [300/469] Loss D: 0.3365 Loss G: 3.0740\n",
            "Epoch [123/200] Batch [400/469] Loss D: 0.3294 Loss G: 3.0682\n",
            "Epoch [124/200] Batch [0/469] Loss D: 0.3121 Loss G: 3.6446\n",
            "Epoch [124/200] Batch [100/469] Loss D: 0.2890 Loss G: 2.3556\n",
            "Epoch [124/200] Batch [200/469] Loss D: 0.3535 Loss G: 2.2282\n",
            "Epoch [124/200] Batch [300/469] Loss D: 0.2296 Loss G: 3.2479\n",
            "Epoch [124/200] Batch [400/469] Loss D: 0.3051 Loss G: 3.0722\n",
            "Epoch [125/200] Batch [0/469] Loss D: 1.3343 Loss G: 4.4673\n",
            "Epoch [125/200] Batch [100/469] Loss D: 0.6192 Loss G: 3.8780\n",
            "Epoch [125/200] Batch [200/469] Loss D: 0.1986 Loss G: 3.7405\n",
            "Epoch [125/200] Batch [300/469] Loss D: 0.2232 Loss G: 2.7920\n",
            "Epoch [125/200] Batch [400/469] Loss D: 0.1803 Loss G: 4.2692\n",
            "Epoch [126/200] Batch [0/469] Loss D: 0.4233 Loss G: 3.5207\n",
            "Epoch [126/200] Batch [100/469] Loss D: 0.4034 Loss G: 3.4530\n",
            "Epoch [126/200] Batch [200/469] Loss D: 0.2733 Loss G: 2.9248\n",
            "Epoch [126/200] Batch [300/469] Loss D: 0.3472 Loss G: 3.5170\n",
            "Epoch [126/200] Batch [400/469] Loss D: 0.3012 Loss G: 3.6420\n",
            "Epoch [127/200] Batch [0/469] Loss D: 0.4078 Loss G: 2.9961\n",
            "Epoch [127/200] Batch [100/469] Loss D: 0.1763 Loss G: 3.8814\n",
            "Epoch [127/200] Batch [200/469] Loss D: 0.3610 Loss G: 4.6333\n",
            "Epoch [127/200] Batch [300/469] Loss D: 0.1684 Loss G: 4.7636\n",
            "Epoch [127/200] Batch [400/469] Loss D: 0.2176 Loss G: 3.4091\n",
            "Epoch [128/200] Batch [0/469] Loss D: 0.1282 Loss G: 3.7699\n",
            "Epoch [128/200] Batch [100/469] Loss D: 0.2916 Loss G: 4.5650\n",
            "Epoch [128/200] Batch [200/469] Loss D: 0.3626 Loss G: 2.9369\n",
            "Epoch [128/200] Batch [300/469] Loss D: 0.2743 Loss G: 3.0866\n",
            "Epoch [128/200] Batch [400/469] Loss D: 0.2825 Loss G: 4.0934\n",
            "Epoch [129/200] Batch [0/469] Loss D: 0.2970 Loss G: 3.4073\n",
            "Epoch [129/200] Batch [100/469] Loss D: 0.2141 Loss G: 3.4910\n",
            "Epoch [129/200] Batch [200/469] Loss D: 0.2747 Loss G: 3.8469\n",
            "Epoch [129/200] Batch [300/469] Loss D: 0.2336 Loss G: 3.5593\n",
            "Epoch [129/200] Batch [400/469] Loss D: 0.3550 Loss G: 4.0765\n",
            "Epoch [130/200] Batch [0/469] Loss D: 0.2102 Loss G: 2.9952\n",
            "Epoch [130/200] Batch [100/469] Loss D: 0.0752 Loss G: 4.0506\n",
            "Epoch [130/200] Batch [200/469] Loss D: 0.3213 Loss G: 2.8274\n",
            "Epoch [130/200] Batch [300/469] Loss D: 0.4148 Loss G: 2.8046\n",
            "Epoch [130/200] Batch [400/469] Loss D: 0.3554 Loss G: 4.2393\n",
            "Epoch [131/200] Batch [0/469] Loss D: 0.1368 Loss G: 3.5959\n",
            "Epoch [131/200] Batch [100/469] Loss D: 0.1848 Loss G: 3.6565\n",
            "Epoch [131/200] Batch [200/469] Loss D: 0.2092 Loss G: 3.7064\n",
            "Epoch [131/200] Batch [300/469] Loss D: 0.2425 Loss G: 3.4746\n",
            "Epoch [131/200] Batch [400/469] Loss D: 0.2740 Loss G: 2.7254\n",
            "Epoch [132/200] Batch [0/469] Loss D: 0.1933 Loss G: 4.7040\n",
            "Epoch [132/200] Batch [100/469] Loss D: 0.4421 Loss G: 3.8144\n",
            "Epoch [132/200] Batch [200/469] Loss D: 0.2070 Loss G: 4.6678\n",
            "Epoch [132/200] Batch [300/469] Loss D: 0.1368 Loss G: 3.8567\n",
            "Epoch [132/200] Batch [400/469] Loss D: 0.9222 Loss G: 1.9728\n",
            "Epoch [133/200] Batch [0/469] Loss D: 0.3310 Loss G: 2.5734\n",
            "Epoch [133/200] Batch [100/469] Loss D: 0.1797 Loss G: 3.6538\n",
            "Epoch [133/200] Batch [200/469] Loss D: 0.1966 Loss G: 3.7275\n",
            "Epoch [133/200] Batch [300/469] Loss D: 0.2307 Loss G: 3.9129\n",
            "Epoch [133/200] Batch [400/469] Loss D: 0.2704 Loss G: 3.4282\n",
            "Epoch [134/200] Batch [0/469] Loss D: 0.2373 Loss G: 3.2036\n",
            "Epoch [134/200] Batch [100/469] Loss D: 0.3872 Loss G: 2.6612\n",
            "Epoch [134/200] Batch [200/469] Loss D: 0.2694 Loss G: 3.3998\n",
            "Epoch [134/200] Batch [300/469] Loss D: 0.2900 Loss G: 4.3503\n",
            "Epoch [134/200] Batch [400/469] Loss D: 0.2428 Loss G: 4.5650\n",
            "Epoch [135/200] Batch [0/469] Loss D: 0.2566 Loss G: 4.3059\n",
            "Epoch [135/200] Batch [100/469] Loss D: 0.3143 Loss G: 2.5012\n",
            "Epoch [135/200] Batch [200/469] Loss D: 0.2739 Loss G: 2.8250\n",
            "Epoch [135/200] Batch [300/469] Loss D: 0.3650 Loss G: 3.9160\n",
            "Epoch [135/200] Batch [400/469] Loss D: 0.1940 Loss G: 4.0057\n",
            "Epoch [136/200] Batch [0/469] Loss D: 0.3089 Loss G: 4.0153\n",
            "Epoch [136/200] Batch [100/469] Loss D: 0.2254 Loss G: 3.1857\n",
            "Epoch [136/200] Batch [200/469] Loss D: 0.5785 Loss G: 2.7803\n",
            "Epoch [136/200] Batch [300/469] Loss D: 0.1070 Loss G: 4.1872\n",
            "Epoch [136/200] Batch [400/469] Loss D: 0.1325 Loss G: 4.0131\n",
            "Epoch [137/200] Batch [0/469] Loss D: 0.3427 Loss G: 4.0798\n",
            "Epoch [137/200] Batch [100/469] Loss D: 0.1950 Loss G: 4.1375\n",
            "Epoch [137/200] Batch [200/469] Loss D: 0.3083 Loss G: 4.4799\n",
            "Epoch [137/200] Batch [300/469] Loss D: 0.6847 Loss G: 1.7119\n",
            "Epoch [137/200] Batch [400/469] Loss D: 0.2963 Loss G: 3.1804\n",
            "Epoch [138/200] Batch [0/469] Loss D: 0.1513 Loss G: 3.9529\n",
            "Epoch [138/200] Batch [100/469] Loss D: 0.1352 Loss G: 4.3063\n",
            "Epoch [138/200] Batch [200/469] Loss D: 0.2591 Loss G: 3.0671\n",
            "Epoch [138/200] Batch [300/469] Loss D: 0.3385 Loss G: 2.6688\n",
            "Epoch [138/200] Batch [400/469] Loss D: 0.3380 Loss G: 3.6360\n",
            "Epoch [139/200] Batch [0/469] Loss D: 0.1856 Loss G: 4.2463\n",
            "Epoch [139/200] Batch [100/469] Loss D: 0.2477 Loss G: 3.2654\n",
            "Epoch [139/200] Batch [200/469] Loss D: 0.3742 Loss G: 5.2458\n",
            "Epoch [139/200] Batch [300/469] Loss D: 0.1370 Loss G: 4.3242\n",
            "Epoch [139/200] Batch [400/469] Loss D: 0.3077 Loss G: 3.7136\n",
            "Epoch [140/200] Batch [0/469] Loss D: 0.3763 Loss G: 2.6319\n",
            "Epoch [140/200] Batch [100/469] Loss D: 0.2760 Loss G: 3.6085\n",
            "Epoch [140/200] Batch [200/469] Loss D: 0.1977 Loss G: 3.7433\n",
            "Epoch [140/200] Batch [300/469] Loss D: 0.2160 Loss G: 4.5168\n",
            "Epoch [140/200] Batch [400/469] Loss D: 0.4674 Loss G: 3.7663\n",
            "Epoch [141/200] Batch [0/469] Loss D: 0.1949 Loss G: 3.1981\n",
            "Epoch [141/200] Batch [100/469] Loss D: 0.3063 Loss G: 2.6842\n",
            "Epoch [141/200] Batch [200/469] Loss D: 0.2720 Loss G: 4.0527\n",
            "Epoch [141/200] Batch [300/469] Loss D: 0.4895 Loss G: 4.9622\n",
            "Epoch [141/200] Batch [400/469] Loss D: 0.1236 Loss G: 5.1108\n",
            "Epoch [142/200] Batch [0/469] Loss D: 0.1645 Loss G: 3.8163\n",
            "Epoch [142/200] Batch [100/469] Loss D: 0.2313 Loss G: 4.3913\n",
            "Epoch [142/200] Batch [200/469] Loss D: 0.1912 Loss G: 2.9207\n",
            "Epoch [142/200] Batch [300/469] Loss D: 0.1803 Loss G: 3.7358\n",
            "Epoch [142/200] Batch [400/469] Loss D: 0.3360 Loss G: 4.9486\n",
            "Epoch [143/200] Batch [0/469] Loss D: 0.3143 Loss G: 2.6462\n",
            "Epoch [143/200] Batch [100/469] Loss D: 0.4107 Loss G: 3.1022\n",
            "Epoch [143/200] Batch [200/469] Loss D: 0.0917 Loss G: 4.3383\n",
            "Epoch [143/200] Batch [300/469] Loss D: 0.1553 Loss G: 3.6285\n",
            "Epoch [143/200] Batch [400/469] Loss D: 0.1604 Loss G: 3.4705\n",
            "Epoch [144/200] Batch [0/469] Loss D: 0.1847 Loss G: 4.0912\n",
            "Epoch [144/200] Batch [100/469] Loss D: 0.3239 Loss G: 3.0466\n",
            "Epoch [144/200] Batch [200/469] Loss D: 0.2224 Loss G: 3.4985\n",
            "Epoch [144/200] Batch [300/469] Loss D: 0.4916 Loss G: 2.6242\n",
            "Epoch [144/200] Batch [400/469] Loss D: 0.2201 Loss G: 3.3161\n",
            "Epoch [145/200] Batch [0/469] Loss D: 0.2658 Loss G: 2.9334\n",
            "Epoch [145/200] Batch [100/469] Loss D: 0.1316 Loss G: 4.5591\n",
            "Epoch [145/200] Batch [200/469] Loss D: 0.2254 Loss G: 4.7441\n",
            "Epoch [145/200] Batch [300/469] Loss D: 0.1073 Loss G: 3.4671\n",
            "Epoch [145/200] Batch [400/469] Loss D: 0.2064 Loss G: 4.0070\n",
            "Epoch [146/200] Batch [0/469] Loss D: 0.1639 Loss G: 3.6168\n",
            "Epoch [146/200] Batch [100/469] Loss D: 0.1408 Loss G: 3.7741\n",
            "Epoch [146/200] Batch [200/469] Loss D: 0.1528 Loss G: 4.6152\n",
            "Epoch [146/200] Batch [300/469] Loss D: 0.2997 Loss G: 2.9926\n",
            "Epoch [146/200] Batch [400/469] Loss D: 0.8690 Loss G: 1.3142\n",
            "Epoch [147/200] Batch [0/469] Loss D: 0.1206 Loss G: 4.3031\n",
            "Epoch [147/200] Batch [100/469] Loss D: 0.2590 Loss G: 3.3002\n",
            "Epoch [147/200] Batch [200/469] Loss D: 0.1310 Loss G: 3.3915\n",
            "Epoch [147/200] Batch [300/469] Loss D: 0.5157 Loss G: 3.9922\n",
            "Epoch [147/200] Batch [400/469] Loss D: 0.1581 Loss G: 4.2689\n",
            "Epoch [148/200] Batch [0/469] Loss D: 0.1797 Loss G: 4.6600\n",
            "Epoch [148/200] Batch [100/469] Loss D: 0.1188 Loss G: 4.8382\n",
            "Epoch [148/200] Batch [200/469] Loss D: 0.2602 Loss G: 3.6263\n",
            "Epoch [148/200] Batch [300/469] Loss D: 0.3022 Loss G: 2.8338\n",
            "Epoch [148/200] Batch [400/469] Loss D: 0.1251 Loss G: 4.1320\n",
            "Epoch [149/200] Batch [0/469] Loss D: 0.1543 Loss G: 3.9564\n",
            "Epoch [149/200] Batch [100/469] Loss D: 0.1974 Loss G: 3.4821\n",
            "Epoch [149/200] Batch [200/469] Loss D: 0.3586 Loss G: 4.6087\n",
            "Epoch [149/200] Batch [300/469] Loss D: 0.4064 Loss G: 4.7613\n",
            "Epoch [149/200] Batch [400/469] Loss D: 0.2716 Loss G: 3.9488\n",
            "Epoch [150/200] Batch [0/469] Loss D: 0.2809 Loss G: 3.4169\n",
            "Epoch [150/200] Batch [100/469] Loss D: 0.1620 Loss G: 3.9768\n",
            "Epoch [150/200] Batch [200/469] Loss D: 0.1786 Loss G: 3.1336\n",
            "Epoch [150/200] Batch [300/469] Loss D: 0.6007 Loss G: 5.3163\n",
            "Epoch [150/200] Batch [400/469] Loss D: 0.1347 Loss G: 3.8993\n",
            "Epoch [151/200] Batch [0/469] Loss D: 0.5898 Loss G: 6.2178\n",
            "Epoch [151/200] Batch [100/469] Loss D: 0.1462 Loss G: 3.7347\n",
            "Epoch [151/200] Batch [200/469] Loss D: 0.2642 Loss G: 3.4087\n",
            "Epoch [151/200] Batch [300/469] Loss D: 0.6957 Loss G: 6.4764\n",
            "Epoch [151/200] Batch [400/469] Loss D: 0.2365 Loss G: 4.6634\n",
            "Epoch [152/200] Batch [0/469] Loss D: 0.0975 Loss G: 4.4120\n",
            "Epoch [152/200] Batch [100/469] Loss D: 0.2822 Loss G: 3.6902\n",
            "Epoch [152/200] Batch [200/469] Loss D: 0.3140 Loss G: 3.1642\n",
            "Epoch [152/200] Batch [300/469] Loss D: 0.1656 Loss G: 4.1671\n",
            "Epoch [152/200] Batch [400/469] Loss D: 0.2108 Loss G: 3.2641\n",
            "Epoch [153/200] Batch [0/469] Loss D: 0.2188 Loss G: 3.8033\n",
            "Epoch [153/200] Batch [100/469] Loss D: 0.2652 Loss G: 5.1187\n",
            "Epoch [153/200] Batch [200/469] Loss D: 0.2057 Loss G: 3.1902\n",
            "Epoch [153/200] Batch [300/469] Loss D: 0.3577 Loss G: 2.7531\n",
            "Epoch [153/200] Batch [400/469] Loss D: 0.5527 Loss G: 5.3084\n",
            "Epoch [154/200] Batch [0/469] Loss D: 0.2208 Loss G: 3.3711\n",
            "Epoch [154/200] Batch [100/469] Loss D: 0.1021 Loss G: 4.5209\n",
            "Epoch [154/200] Batch [200/469] Loss D: 0.3528 Loss G: 4.4969\n",
            "Epoch [154/200] Batch [300/469] Loss D: 0.2159 Loss G: 4.5533\n",
            "Epoch [154/200] Batch [400/469] Loss D: 1.3912 Loss G: 1.0799\n",
            "Epoch [155/200] Batch [0/469] Loss D: 0.3673 Loss G: 4.7777\n",
            "Epoch [155/200] Batch [100/469] Loss D: 0.2335 Loss G: 3.1697\n",
            "Epoch [155/200] Batch [200/469] Loss D: 0.2110 Loss G: 3.6988\n",
            "Epoch [155/200] Batch [300/469] Loss D: 0.6541 Loss G: 7.2148\n",
            "Epoch [155/200] Batch [400/469] Loss D: 0.2846 Loss G: 3.8169\n",
            "Epoch [156/200] Batch [0/469] Loss D: 0.3472 Loss G: 4.1234\n",
            "Epoch [156/200] Batch [100/469] Loss D: 0.2052 Loss G: 2.9898\n",
            "Epoch [156/200] Batch [200/469] Loss D: 0.2489 Loss G: 3.7986\n",
            "Epoch [156/200] Batch [300/469] Loss D: 0.2120 Loss G: 5.0964\n",
            "Epoch [156/200] Batch [400/469] Loss D: 0.9799 Loss G: 6.8655\n",
            "Epoch [157/200] Batch [0/469] Loss D: 0.3216 Loss G: 3.7721\n",
            "Epoch [157/200] Batch [100/469] Loss D: 0.2475 Loss G: 3.8208\n",
            "Epoch [157/200] Batch [200/469] Loss D: 0.2730 Loss G: 3.2780\n",
            "Epoch [157/200] Batch [300/469] Loss D: 0.1078 Loss G: 4.6193\n",
            "Epoch [157/200] Batch [400/469] Loss D: 0.1916 Loss G: 3.7136\n",
            "Epoch [158/200] Batch [0/469] Loss D: 1.2040 Loss G: 7.0515\n",
            "Epoch [158/200] Batch [100/469] Loss D: 0.1699 Loss G: 3.8222\n",
            "Epoch [158/200] Batch [200/469] Loss D: 0.2166 Loss G: 3.8255\n",
            "Epoch [158/200] Batch [300/469] Loss D: 0.1349 Loss G: 4.0245\n",
            "Epoch [158/200] Batch [400/469] Loss D: 0.1902 Loss G: 3.7414\n",
            "Epoch [159/200] Batch [0/469] Loss D: 0.5876 Loss G: 1.5811\n",
            "Epoch [159/200] Batch [100/469] Loss D: 0.2440 Loss G: 4.4567\n",
            "Epoch [159/200] Batch [200/469] Loss D: 0.1368 Loss G: 4.0468\n",
            "Epoch [159/200] Batch [300/469] Loss D: 0.1316 Loss G: 4.2423\n",
            "Epoch [159/200] Batch [400/469] Loss D: 0.1339 Loss G: 4.5954\n",
            "Epoch [160/200] Batch [0/469] Loss D: 0.2146 Loss G: 4.4504\n",
            "Epoch [160/200] Batch [100/469] Loss D: 0.1998 Loss G: 4.8523\n",
            "Epoch [160/200] Batch [200/469] Loss D: 0.0875 Loss G: 4.6166\n",
            "Epoch [160/200] Batch [300/469] Loss D: 0.2540 Loss G: 3.8432\n",
            "Epoch [160/200] Batch [400/469] Loss D: 0.4326 Loss G: 5.4725\n",
            "Epoch [161/200] Batch [0/469] Loss D: 0.1966 Loss G: 4.2629\n",
            "Epoch [161/200] Batch [100/469] Loss D: 0.0812 Loss G: 4.1981\n",
            "Epoch [161/200] Batch [200/469] Loss D: 0.2447 Loss G: 4.0532\n",
            "Epoch [161/200] Batch [300/469] Loss D: 0.1194 Loss G: 4.5755\n",
            "Epoch [161/200] Batch [400/469] Loss D: 0.2566 Loss G: 5.0836\n",
            "Epoch [162/200] Batch [0/469] Loss D: 0.1134 Loss G: 4.1828\n",
            "Epoch [162/200] Batch [100/469] Loss D: 0.0800 Loss G: 4.8380\n",
            "Epoch [162/200] Batch [200/469] Loss D: 0.1555 Loss G: 3.7153\n",
            "Epoch [162/200] Batch [300/469] Loss D: 2.1718 Loss G: 0.9870\n",
            "Epoch [162/200] Batch [400/469] Loss D: 0.1648 Loss G: 2.6988\n",
            "Epoch [163/200] Batch [0/469] Loss D: 0.3495 Loss G: 5.0459\n",
            "Epoch [163/200] Batch [100/469] Loss D: 0.2418 Loss G: 3.4925\n",
            "Epoch [163/200] Batch [200/469] Loss D: 0.1947 Loss G: 3.8972\n",
            "Epoch [163/200] Batch [300/469] Loss D: 0.1387 Loss G: 4.7457\n",
            "Epoch [163/200] Batch [400/469] Loss D: 0.2132 Loss G: 4.5270\n",
            "Epoch [164/200] Batch [0/469] Loss D: 0.1361 Loss G: 4.7501\n",
            "Epoch [164/200] Batch [100/469] Loss D: 0.2463 Loss G: 2.7267\n",
            "Epoch [164/200] Batch [200/469] Loss D: 0.1742 Loss G: 3.8362\n",
            "Epoch [164/200] Batch [300/469] Loss D: 0.1544 Loss G: 4.3246\n",
            "Epoch [164/200] Batch [400/469] Loss D: 0.1097 Loss G: 4.3386\n",
            "Epoch [165/200] Batch [0/469] Loss D: 0.2836 Loss G: 4.8210\n",
            "Epoch [165/200] Batch [100/469] Loss D: 0.1510 Loss G: 3.7341\n",
            "Epoch [165/200] Batch [200/469] Loss D: 0.1024 Loss G: 4.3534\n",
            "Epoch [165/200] Batch [300/469] Loss D: 0.1920 Loss G: 5.1588\n",
            "Epoch [165/200] Batch [400/469] Loss D: 0.2049 Loss G: 3.6654\n",
            "Epoch [166/200] Batch [0/469] Loss D: 0.1828 Loss G: 4.6470\n",
            "Epoch [166/200] Batch [100/469] Loss D: 0.1572 Loss G: 4.1073\n",
            "Epoch [166/200] Batch [200/469] Loss D: 0.1519 Loss G: 4.1002\n",
            "Epoch [166/200] Batch [300/469] Loss D: 0.1860 Loss G: 4.8625\n",
            "Epoch [166/200] Batch [400/469] Loss D: 0.1372 Loss G: 4.5873\n",
            "Epoch [167/200] Batch [0/469] Loss D: 0.1360 Loss G: 4.5697\n",
            "Epoch [167/200] Batch [100/469] Loss D: 1.1570 Loss G: 5.5566\n",
            "Epoch [167/200] Batch [200/469] Loss D: 0.3270 Loss G: 4.0332\n",
            "Epoch [167/200] Batch [300/469] Loss D: 0.3459 Loss G: 4.0433\n",
            "Epoch [167/200] Batch [400/469] Loss D: 0.0836 Loss G: 4.6479\n",
            "Epoch [168/200] Batch [0/469] Loss D: 0.0867 Loss G: 4.5121\n",
            "Epoch [168/200] Batch [100/469] Loss D: 0.0577 Loss G: 5.0278\n",
            "Epoch [168/200] Batch [200/469] Loss D: 0.1100 Loss G: 4.1549\n",
            "Epoch [168/200] Batch [300/469] Loss D: 0.0892 Loss G: 3.7536\n",
            "Epoch [168/200] Batch [400/469] Loss D: 0.2370 Loss G: 3.6859\n",
            "Epoch [169/200] Batch [0/469] Loss D: 0.1659 Loss G: 3.8938\n",
            "Epoch [169/200] Batch [100/469] Loss D: 0.0780 Loss G: 4.4483\n",
            "Epoch [169/200] Batch [200/469] Loss D: 0.1645 Loss G: 4.1244\n",
            "Epoch [169/200] Batch [300/469] Loss D: 0.2075 Loss G: 3.3477\n",
            "Epoch [169/200] Batch [400/469] Loss D: 0.2856 Loss G: 1.9894\n",
            "Epoch [170/200] Batch [0/469] Loss D: 0.2404 Loss G: 5.3468\n",
            "Epoch [170/200] Batch [100/469] Loss D: 0.1517 Loss G: 4.6140\n",
            "Epoch [170/200] Batch [200/469] Loss D: 3.5740 Loss G: 4.6757\n",
            "Epoch [170/200] Batch [300/469] Loss D: 0.2523 Loss G: 3.5099\n",
            "Epoch [170/200] Batch [400/469] Loss D: 0.1363 Loss G: 4.1167\n",
            "Epoch [171/200] Batch [0/469] Loss D: 0.0635 Loss G: 4.8700\n",
            "Epoch [171/200] Batch [100/469] Loss D: 0.1206 Loss G: 3.8532\n",
            "Epoch [171/200] Batch [200/469] Loss D: 0.1575 Loss G: 3.6133\n",
            "Epoch [171/200] Batch [300/469] Loss D: 0.1470 Loss G: 4.4017\n",
            "Epoch [171/200] Batch [400/469] Loss D: 0.4730 Loss G: 4.6699\n",
            "Epoch [172/200] Batch [0/469] Loss D: 0.1100 Loss G: 3.7390\n",
            "Epoch [172/200] Batch [100/469] Loss D: 0.3570 Loss G: 4.8899\n",
            "Epoch [172/200] Batch [200/469] Loss D: 0.2423 Loss G: 3.8538\n",
            "Epoch [172/200] Batch [300/469] Loss D: 0.3867 Loss G: 2.1926\n",
            "Epoch [172/200] Batch [400/469] Loss D: 0.3711 Loss G: 2.6833\n",
            "Epoch [173/200] Batch [0/469] Loss D: 0.1338 Loss G: 5.7660\n",
            "Epoch [173/200] Batch [100/469] Loss D: 0.5029 Loss G: 6.7146\n",
            "Epoch [173/200] Batch [200/469] Loss D: 0.1705 Loss G: 5.1185\n",
            "Epoch [173/200] Batch [300/469] Loss D: 0.1811 Loss G: 3.9729\n",
            "Epoch [173/200] Batch [400/469] Loss D: 0.0949 Loss G: 4.5504\n",
            "Epoch [174/200] Batch [0/469] Loss D: 0.1467 Loss G: 3.8924\n",
            "Epoch [174/200] Batch [100/469] Loss D: 0.2369 Loss G: 4.3846\n",
            "Epoch [174/200] Batch [200/469] Loss D: 0.1324 Loss G: 4.6706\n",
            "Epoch [174/200] Batch [300/469] Loss D: 0.1964 Loss G: 4.1060\n",
            "Epoch [174/200] Batch [400/469] Loss D: 0.2072 Loss G: 4.2677\n",
            "Epoch [175/200] Batch [0/469] Loss D: 0.2595 Loss G: 5.2603\n",
            "Epoch [175/200] Batch [100/469] Loss D: 0.1293 Loss G: 4.1720\n",
            "Epoch [175/200] Batch [200/469] Loss D: 0.2001 Loss G: 4.1252\n",
            "Epoch [175/200] Batch [300/469] Loss D: 0.1535 Loss G: 4.2947\n",
            "Epoch [175/200] Batch [400/469] Loss D: 0.1768 Loss G: 4.3076\n",
            "Epoch [176/200] Batch [0/469] Loss D: 0.2625 Loss G: 4.9747\n",
            "Epoch [176/200] Batch [100/469] Loss D: 0.2040 Loss G: 4.9667\n",
            "Epoch [176/200] Batch [200/469] Loss D: 0.1634 Loss G: 4.7058\n",
            "Epoch [176/200] Batch [300/469] Loss D: 0.0604 Loss G: 4.1443\n",
            "Epoch [176/200] Batch [400/469] Loss D: 0.1642 Loss G: 3.0015\n",
            "Epoch [177/200] Batch [0/469] Loss D: 0.1618 Loss G: 4.3997\n",
            "Epoch [177/200] Batch [100/469] Loss D: 0.1343 Loss G: 3.9529\n",
            "Epoch [177/200] Batch [200/469] Loss D: 0.1510 Loss G: 3.7062\n",
            "Epoch [177/200] Batch [300/469] Loss D: 0.0687 Loss G: 5.8353\n",
            "Epoch [177/200] Batch [400/469] Loss D: 0.1985 Loss G: 3.8705\n",
            "Epoch [178/200] Batch [0/469] Loss D: 0.0865 Loss G: 4.8608\n",
            "Epoch [178/200] Batch [100/469] Loss D: 0.1339 Loss G: 3.7486\n",
            "Epoch [178/200] Batch [200/469] Loss D: 0.1827 Loss G: 3.7066\n",
            "Epoch [178/200] Batch [300/469] Loss D: 0.1754 Loss G: 3.4203\n",
            "Epoch [178/200] Batch [400/469] Loss D: 0.5005 Loss G: 2.9883\n",
            "Epoch [179/200] Batch [0/469] Loss D: 0.3507 Loss G: 6.0123\n",
            "Epoch [179/200] Batch [100/469] Loss D: 0.1480 Loss G: 4.0417\n",
            "Epoch [179/200] Batch [200/469] Loss D: 0.1779 Loss G: 3.5475\n",
            "Epoch [179/200] Batch [300/469] Loss D: 0.1042 Loss G: 4.1435\n",
            "Epoch [179/200] Batch [400/469] Loss D: 0.2272 Loss G: 4.3837\n",
            "Epoch [180/200] Batch [0/469] Loss D: 0.1895 Loss G: 4.5863\n",
            "Epoch [180/200] Batch [100/469] Loss D: 0.0894 Loss G: 5.2325\n",
            "Epoch [180/200] Batch [200/469] Loss D: 0.2696 Loss G: 4.5533\n",
            "Epoch [180/200] Batch [300/469] Loss D: 3.1337 Loss G: 0.0363\n",
            "Epoch [180/200] Batch [400/469] Loss D: 0.2305 Loss G: 3.9394\n",
            "Epoch [181/200] Batch [0/469] Loss D: 0.7384 Loss G: 6.4730\n",
            "Epoch [181/200] Batch [100/469] Loss D: 0.0507 Loss G: 4.8645\n",
            "Epoch [181/200] Batch [200/469] Loss D: 0.1059 Loss G: 4.0462\n",
            "Epoch [181/200] Batch [300/469] Loss D: 0.1495 Loss G: 3.7298\n",
            "Epoch [181/200] Batch [400/469] Loss D: 0.1663 Loss G: 4.9629\n",
            "Epoch [182/200] Batch [0/469] Loss D: 0.1172 Loss G: 4.4228\n",
            "Epoch [182/200] Batch [100/469] Loss D: 0.1008 Loss G: 5.1205\n",
            "Epoch [182/200] Batch [200/469] Loss D: 0.2266 Loss G: 3.1606\n",
            "Epoch [182/200] Batch [300/469] Loss D: 0.0763 Loss G: 4.1314\n",
            "Epoch [182/200] Batch [400/469] Loss D: 0.1403 Loss G: 4.8354\n",
            "Epoch [183/200] Batch [0/469] Loss D: 0.1399 Loss G: 4.7702\n",
            "Epoch [183/200] Batch [100/469] Loss D: 0.1930 Loss G: 3.7321\n",
            "Epoch [183/200] Batch [200/469] Loss D: 0.1856 Loss G: 4.6492\n",
            "Epoch [183/200] Batch [300/469] Loss D: 0.1265 Loss G: 4.7620\n",
            "Epoch [183/200] Batch [400/469] Loss D: 0.2345 Loss G: 4.0009\n",
            "Epoch [184/200] Batch [0/469] Loss D: 0.0972 Loss G: 4.3629\n",
            "Epoch [184/200] Batch [100/469] Loss D: 0.1464 Loss G: 3.9719\n",
            "Epoch [184/200] Batch [200/469] Loss D: 0.1120 Loss G: 4.2338\n",
            "Epoch [184/200] Batch [300/469] Loss D: 0.1850 Loss G: 4.5340\n",
            "Epoch [184/200] Batch [400/469] Loss D: 0.2530 Loss G: 3.7951\n",
            "Epoch [185/200] Batch [0/469] Loss D: 0.0920 Loss G: 4.5279\n",
            "Epoch [185/200] Batch [100/469] Loss D: 0.2311 Loss G: 4.1724\n",
            "Epoch [185/200] Batch [200/469] Loss D: 0.2443 Loss G: 3.5492\n",
            "Epoch [185/200] Batch [300/469] Loss D: 0.2035 Loss G: 3.7893\n",
            "Epoch [185/200] Batch [400/469] Loss D: 0.1803 Loss G: 3.6153\n",
            "Epoch [186/200] Batch [0/469] Loss D: 0.1463 Loss G: 3.9447\n",
            "Epoch [186/200] Batch [100/469] Loss D: 0.1169 Loss G: 4.4340\n",
            "Epoch [186/200] Batch [200/469] Loss D: 0.1559 Loss G: 4.5554\n",
            "Epoch [186/200] Batch [300/469] Loss D: 0.2703 Loss G: 4.7766\n",
            "Epoch [186/200] Batch [400/469] Loss D: 0.1347 Loss G: 4.1006\n",
            "Epoch [187/200] Batch [0/469] Loss D: 0.3525 Loss G: 2.5497\n",
            "Epoch [187/200] Batch [100/469] Loss D: 0.1546 Loss G: 4.1882\n",
            "Epoch [187/200] Batch [200/469] Loss D: 0.2620 Loss G: 2.8734\n",
            "Epoch [187/200] Batch [300/469] Loss D: 0.1246 Loss G: 4.3526\n",
            "Epoch [187/200] Batch [400/469] Loss D: 0.1258 Loss G: 4.3343\n",
            "Epoch [188/200] Batch [0/469] Loss D: 0.1460 Loss G: 4.7121\n",
            "Epoch [188/200] Batch [100/469] Loss D: 0.2950 Loss G: 6.1449\n",
            "Epoch [188/200] Batch [200/469] Loss D: 0.2143 Loss G: 5.7782\n",
            "Epoch [188/200] Batch [300/469] Loss D: 0.0547 Loss G: 4.5521\n",
            "Epoch [188/200] Batch [400/469] Loss D: 0.0971 Loss G: 4.1733\n",
            "Epoch [189/200] Batch [0/469] Loss D: 0.2540 Loss G: 4.0214\n",
            "Epoch [189/200] Batch [100/469] Loss D: 0.1720 Loss G: 3.9496\n",
            "Epoch [189/200] Batch [200/469] Loss D: 0.1019 Loss G: 4.2890\n",
            "Epoch [189/200] Batch [300/469] Loss D: 0.2753 Loss G: 5.1104\n",
            "Epoch [189/200] Batch [400/469] Loss D: 0.0892 Loss G: 4.7471\n",
            "Epoch [190/200] Batch [0/469] Loss D: 0.1824 Loss G: 3.9921\n",
            "Epoch [190/200] Batch [100/469] Loss D: 0.3852 Loss G: 2.3093\n",
            "Epoch [190/200] Batch [200/469] Loss D: 0.1399 Loss G: 5.0908\n",
            "Epoch [190/200] Batch [300/469] Loss D: 0.1222 Loss G: 4.6482\n",
            "Epoch [190/200] Batch [400/469] Loss D: 0.0899 Loss G: 3.9211\n",
            "Epoch [191/200] Batch [0/469] Loss D: 0.0908 Loss G: 5.4050\n",
            "Epoch [191/200] Batch [100/469] Loss D: 0.1275 Loss G: 6.4372\n",
            "Epoch [191/200] Batch [200/469] Loss D: 0.1917 Loss G: 3.3737\n",
            "Epoch [191/200] Batch [300/469] Loss D: 0.1110 Loss G: 5.0338\n",
            "Epoch [191/200] Batch [400/469] Loss D: 0.0551 Loss G: 5.1539\n",
            "Epoch [192/200] Batch [0/469] Loss D: 0.2826 Loss G: 5.3497\n",
            "Epoch [192/200] Batch [100/469] Loss D: 0.1086 Loss G: 4.8659\n",
            "Epoch [192/200] Batch [200/469] Loss D: 0.0704 Loss G: 5.1005\n",
            "Epoch [192/200] Batch [300/469] Loss D: 0.1430 Loss G: 5.2846\n",
            "Epoch [192/200] Batch [400/469] Loss D: 0.1148 Loss G: 5.2403\n",
            "Epoch [193/200] Batch [0/469] Loss D: 0.3906 Loss G: 6.0980\n",
            "Epoch [193/200] Batch [100/469] Loss D: 0.1782 Loss G: 4.1975\n",
            "Epoch [193/200] Batch [200/469] Loss D: 0.0749 Loss G: 5.4384\n",
            "Epoch [193/200] Batch [300/469] Loss D: 0.1591 Loss G: 4.8069\n",
            "Epoch [193/200] Batch [400/469] Loss D: 0.1025 Loss G: 4.6485\n",
            "Epoch [194/200] Batch [0/469] Loss D: 0.1586 Loss G: 4.8592\n",
            "Epoch [194/200] Batch [100/469] Loss D: 0.1005 Loss G: 4.6448\n",
            "Epoch [194/200] Batch [200/469] Loss D: 0.1384 Loss G: 5.4460\n",
            "Epoch [194/200] Batch [300/469] Loss D: 0.0562 Loss G: 5.5093\n",
            "Epoch [194/200] Batch [400/469] Loss D: 0.4259 Loss G: 1.8635\n",
            "Epoch [195/200] Batch [0/469] Loss D: 0.3181 Loss G: 4.2032\n",
            "Epoch [195/200] Batch [100/469] Loss D: 0.2203 Loss G: 3.8009\n",
            "Epoch [195/200] Batch [200/469] Loss D: 0.0878 Loss G: 4.3644\n",
            "Epoch [195/200] Batch [300/469] Loss D: 0.0986 Loss G: 4.5522\n",
            "Epoch [195/200] Batch [400/469] Loss D: 0.2210 Loss G: 5.3178\n",
            "Epoch [196/200] Batch [0/469] Loss D: 0.1848 Loss G: 4.2917\n",
            "Epoch [196/200] Batch [100/469] Loss D: 0.1375 Loss G: 4.0094\n",
            "Epoch [196/200] Batch [200/469] Loss D: 0.1830 Loss G: 4.5698\n",
            "Epoch [196/200] Batch [300/469] Loss D: 0.1707 Loss G: 4.4447\n",
            "Epoch [196/200] Batch [400/469] Loss D: 0.0899 Loss G: 4.8092\n",
            "Epoch [197/200] Batch [0/469] Loss D: 0.0425 Loss G: 5.0850\n",
            "Epoch [197/200] Batch [100/469] Loss D: 0.1139 Loss G: 4.8433\n",
            "Epoch [197/200] Batch [200/469] Loss D: 0.1261 Loss G: 5.0388\n",
            "Epoch [197/200] Batch [300/469] Loss D: 0.1021 Loss G: 4.3883\n",
            "Epoch [197/200] Batch [400/469] Loss D: 0.1043 Loss G: 4.4584\n",
            "Epoch [198/200] Batch [0/469] Loss D: 0.3591 Loss G: 2.5410\n",
            "Epoch [198/200] Batch [100/469] Loss D: 0.2578 Loss G: 2.3369\n",
            "Epoch [198/200] Batch [200/469] Loss D: 0.1033 Loss G: 4.5431\n",
            "Epoch [198/200] Batch [300/469] Loss D: 0.0987 Loss G: 4.5269\n",
            "Epoch [198/200] Batch [400/469] Loss D: 0.1916 Loss G: 4.9457\n",
            "Epoch [199/200] Batch [0/469] Loss D: 0.5578 Loss G: 6.5981\n",
            "Epoch [199/200] Batch [100/469] Loss D: 0.0837 Loss G: 5.8503\n",
            "Epoch [199/200] Batch [200/469] Loss D: 0.1673 Loss G: 4.6201\n",
            "Epoch [199/200] Batch [300/469] Loss D: 0.2084 Loss G: 4.0292\n",
            "Epoch [199/200] Batch [400/469] Loss D: 0.1339 Loss G: 3.7181\n",
            "Epoch [200/200] Batch [0/469] Loss D: 2.7808 Loss G: 13.5036\n",
            "Epoch [200/200] Batch [100/469] Loss D: 0.2638 Loss G: 2.9503\n",
            "Epoch [200/200] Batch [200/469] Loss D: 0.1785 Loss G: 4.7240\n",
            "Epoch [200/200] Batch [300/469] Loss D: 0.0886 Loss G: 3.7851\n",
            "Epoch [200/200] Batch [400/469] Loss D: 0.6267 Loss G: 5.9196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Save the final trained Generator model ---\n",
        "model_save_path = \"generator_final.pth\"\n",
        "torch.save(netG.state_dict(), model_save_path)\n",
        "print(f\"Training complete. Final generator model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6NqphAXGHUu",
        "outputId": "47084024-f593-44fb-c0ea-7cf50a39fe3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete. Final generator model saved to generator_final.pth\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}